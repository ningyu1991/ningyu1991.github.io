<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="keywords" content="Ning Yu; 于宁; Computer Vision; Visual Security; Deep Generative Modeling; University of Maryland; UMD; Max Planck Institute for Informatics; MPI-INF">
<link rel="author" href="https://ningyu1991.github.io/">

<title>Ning Yu's Homepage</title>

<style>
@media screen and (max-device-width: 480px){
  body{
    -webkit-text-size-adjust: none;
  }
}
p { font-size : 16px; }
h1 { font-size : 34px; margin : 0; padding : 0; }
h2 { font-size : 20px; margin : 0; padding : 0; }
h3 { font-size : 18px; margin : 8; padding : 0; }
body { padding : 0; font-family : Arial; font-size : 16px; background-color : #fff; }
.title { width : 700px; margin : 20px auto; }
.container { width : 900px; margin : 20px auto; border-radius: 10px;  background-color : #fff; padding : 20px;  clear:both;}
#bio {
    padding-top : 30px;
}
#me { border : 0 solid black; margin-bottom : 50px; border-radius : 10px; }
#sidebar { margin-left : 25px; border : 0 solid black; float : right; margin-bottom : 0;}
a { text-decoration : none; }
a:hover { text-decoration : underline; }
a, a:visited { color : #0050e7; }
.publogo { width: 100 px; margin-right : 20px; float : left; border : 0;}
.publication { clear : left; padding-bottom : 0px; }
.publication p { height : 100px; padding-top : 5px;}
.publication strong a { color : #0000A0; }
.publication .links a { margin-right : 20px; }
.codelogo { margin-right : 10px; float : left; border : 0;}
.code { clear : left; padding-bottom : 10px; vertical-align :middle;} 
.code .download a { display : block; margin : 0 15px; float : left;}
.code strong a { color : #000; }
.external a { margin : 0 10px; }
.external a.first { margin : 0 10px 0 0; }
</style>

<script async="" src="./homepage_files/analytics.js"></script>
</head>

<body>
    <div class="title">
        <div id="sidebar"><img src="./homepage_files/me.jpg" vspace="50 px" width="270 px" id="me" itemprop="photo"></div>
        <div id="bio">

            <br>

            <h1>
                <span itemprop="name">Ning Yu <font size="5">于宁</font> </span>
            </h1>

            <br>

            <p style="line-height:23px;">
                Research Scientist
                <br>
                Salesforce Research
                <br>
                ning.yu at salesforce.com
                <br>
                <br>
            </p>
            <p class="external">
                <a href="https://scholar.google.com/citations?user=TaJND9YAAAAJ&hl=en" class="first" target="_blank" rel="nofollow">Google Scholar</a>|
                <a href="https://github.com/ningyu1991" target="_blank" rel="nofollow">GitHub</a>|
                <a href="https://www.linkedin.com/in/ning-yu-51b31087" target="_blank" rel="nofollow">LinkedIn</a>|
                <a href="https://twitter.com/realNingYu" target="_blank" rel="nofollow">Twitter</a>
                <br>
                <br>
                <a href="#sect-news" class="first">News</a>|
                <a href="#sect-publications">Publications</a>|
                <a href="#sect-patents">Patents</a>|
                <a href="#sect-awards">Awards</a>
                <a href="#sect-mentoring" class="first">Mentoring</a>|
                <a href="#sect-chairing" class="first">Chairing</a>|
                <a href="#sect-reviewing">Reviewing</a>|
                <a href="#sect-teaching">Teaching</a>
            </p>
        </div>
    </div>

    <div class="container">
        <p>
            <h2>Call for interns</h2>
            <br>
            I am looking for Ph.D. research interns with a background in visual generative models or vision-language learning during spring/summer 2023 and beyond. Each internship targets 1+ top-tier publications, has the potential to transfer to real features in Salesforce AI-empowered products, and is followed by full-time opportunities. Please apply <a href="https://salesforce.wd1.myworkdayjobs.com/en-US/External_Career_Site/job/California---Palo-Alto/XMLNAME-2023-Research-Intern---Salesforce-Research-Tableau-Research_JR158577-2?source=LinkedIn_Jobs" target="_blank" rel="nofollow">here</a> and email me your C.V. The hiring is <b>rolling-based</b>.
        </p>
    </div>

    <div class="container">
        <p>
            <h2>About me</h2>
            <br>
            I am a Research Scientist at Salesforce Research. My research aspirations lie in computer vision and visual security, with a focused lens at the bright/dark sides of multimodal generative models and vision-language learning. I hold a Ph.D. from the joint program with the University of Maryland and Max Planck Institute for Informatics where I was supervised by <a href="https://lsd.umiacs.io/" target="_blank" rel="nofollow">Larry Davis</a> and <a href="https://cispa.saarland/group/fritz/" target="_blank" rel="nofollow">Mario Fritz</a>.
            <br>
            <br>
            My selected works include <a href="https://canqin001.github.io/UniControl-Page/" target="_blank" rel="nofollow">UniControl</a>, <a href="https://github.com/salesforce/ULIP" target="_blank" rel="nofollow">ULIP-2</a>, <a href="./projects/LayoutDETR.html" target="_blank" rel="nofollow">LayoutDETR</a>, <a href="https://canqin001.github.io/GlueGen-Page" target="_blank" rel="nofollow">GlueGen</a>, <a href="https://shugerdou.github.io/hive/" target="_blank" rel="nofollow">HIVE</a>, <a href="https://arxiv.org/pdf/2103.16748.pdf" target="_blank" rel="nofollow">Contrast and Attention GANs</a>, <a href="./projects/ScalableGANFingerprints.html" target="_blank" rel="nofollow">Scalable GAN Fingerprints</a>, <a href="./projects/ArtificialGANFingerprints.html" target="_blank" rel="nofollow">Artificial GAN Fingerprints</a>, <a href="./projects/GANFingerprints.html" target="_blank" rel="nofollow">GAN Fingerprints</a>, <a href="./projects/InclusiveGAN.html" target="_blank" rel="nofollow">Inclusive GAN</a>, <a href="./projects/TextureMixer.html" target="_blank" rel="nofollow">Texture Mixer</a>, and <a href="./projects/DefectDetection.html" target="_blank" rel="nofollow">Defect Detection</a>. My research was featured in media press such as AlmostHuman, AIera, JiangMen, QbitAI, and FoodPlus.
            <br>
            <br>
            I am constantly dedicated to academic services, such as serving as area chair for CVPR, ICCV, NeurIPS, AAAI, AISTATS, WACV, and BMVC.
            <br>
            <br>
            I am a recipient of <a href="https://blog.twitch.tv/en/2021/01/07/introducing-our-2021-twitch-research-fellows/" target="_blank" rel="nofollow">Twitch (Amazon) Research Fellowship</a>, Microsoft Young Fellowship, <a href="https://www.qualcomm.com/invention/research/university-relations/innovation-fellowship/finalists" target="_blank" rel="nofollow">Qualcomm Innovation Fellowship Finalist x2</a>, and <a href="https://web.archive.org/web/20210422170343/http://spie.org/about-spie/press-room/mi15-news" target="_blank" rel="nofollow">SPIE Best Student Paper Finalist</a>.
        </p>
    </div>

    <div class="container">
        <p id="sect-news">
            <h2>News</h2>
            <br>
            [2023/05] <a href="https://arxiv.org/pdf/2210.06998.pdf" target="_blank" rel="nofollow">DE-FAKE</a> is accepted to CCS 2023.
            <br>
            [2023/04] <a href="https://arxiv.org/pdf/2306.07758.pdf" target="_blank" rel="nofollow">Generated Graph Detection</a> is accepted to ICML 2023. The <a href="https://github.com/Yvonnemamama/GGD" target="_blank" rel="nofollow">code</a> is released.
            <br>
            [2023/04] <a href="https://arxiv.org/pdf/2304.11359.pdf" target="_blank" rel="nofollow">SPAD</a> and <a href="https://arxiv.org/pdf/2302.00491.pdf" target="_blank" rel="nofollow">Prototype LTR</a> are accepted to IJCAI 2023.
            <br>
            [2023/04] <a href="https://arxiv.org/pdf/2304.03400.pdf" target="_blank" rel="nofollow">RoSteALS</a> is accepted to CVPR Workshop on Media Forensics 2023. The <a href="https://github.com/TuBui/RoSteALS" target="_blank" rel="nofollow">code</a> is released.
            <br>
            [2023/02] <a href="https://arxiv.org/pdf/2303.16891.pdf" target="_blank" rel="nofollow">Mask-free OVIS</a> and <a href="https://arxiv.org/pdf/2201.07513.pdf" target="_blank" rel="nofollow">Cont-Steal</a> are accepted to CVPR 2023.
            <br>
            [2022/10] <a href="https://arxiv.org/pdf/2210.00957.pdf" target="_blank" rel="nofollow">UnGANable</a> is accepted to USENIX 2023. The <a href="https://github.com/zhenglisec/UnGANable" target="_blank" rel="nofollow">code</a> is released.
            <br>
            [2022/08] <a href="https://arxiv.org/pdf/2208.03382.pdf" target="_blank" rel="nofollow">FcF Image Inpainting</a> is accepted to WACV 2023. The <a href="https://github.com/SHI-Labs/FcF-Inpainting" target="_blank" rel="nofollow">code</a> is released.
            <br>
            [2022/07] I am one of the <a href="https://icml.cc/Conferences/2022/Reviewers" target="_blank" rel="nofollow">top reviewers</a> and serve as a session chair for ICML 2022.
            <br>
            [2022/07] <a href="https://arxiv.org/pdf/2207.02063.pdf" target="_blank" rel="nofollow">RepMix</a> is accepted to ECCV 2022. The <a href="https://github.com/TuBui/image_attribution" target="_blank" rel="nofollow">code</a> is released.
            <br>
            [2022/05] <a href="https://www.sciencedirect.com/science/article/pii/S0308814622012055" target="_blank" rel="nofollow">Vision-Language for Food Nutrient Estimation</a> is accepted to Food Chemistry 2022.
            <br>
            [2022/04] <a href="https://arxiv.org/pdf/2208.11180.pdf" target="_blank" rel="nofollow">Multi-Exit Privacy</a> is accepted to CCS 2022. The <a href="https://github.com/zhenglisec/Multi-Exit-Privacy" target="_blank" rel="nofollow">code</a> is released.
            <br>
            [2022/04] The code for <a href="https://github.com/DingfanChen/RelaxLoss" target="_blank" rel="nofollow">RelaxLoss</a> is released.
            <br>
            [2022/01] The code for <a href="https://github.com/ningyu1991/ScalableGANFingerprints" target="_blank" rel="nofollow">Scalable GAN Fingerprints</a> is released.
            <br>
            [2022/01] <a href="https://arxiv.org/pdf/2012.08726.pdf" target="_blank" rel="nofollow">Scalable GAN Fingerprints</a> and <a href="https://openreview.net/pdf?id=FEDfGWVZYIn" target="_blank" rel="nofollow">RelaxLoss</a> are accepted to ICLR 2022 as <strong style="color: red;">Spotlight</strong>.
            <br>
            [2021/10] <a href="https://arxiv.org/pdf/2101.11080.pdf" target="_blank" rel="nofollow">VIDNet</a> is accepted to BMVC 2021. The <a href="https://github.com/pengzhou1108/VIDNet" target="_blank" rel="nofollow">code</a> is released.
            <br>
            [2021/08] I defended my Ph.D. The dissertation about <a href="https://drum.lib.umd.edu/handle/1903/27856" target="_blank" rel="nofollow">Human-Centric Deep Generative Models</a> is online.
            <br>
            [2021/08] <a href="https://www.sciencedirect.com/science/article/abs/pii/S0308814621020008" target="_blank" rel="nofollow">CV for Food Nutrient Estimation</a> is accepted to Food Chemistry 2021.
            <br>
            [2021/07] <a href="https://arxiv.org/pdf/2007.08457.pdf" target="_blank" rel="nofollow">Artificial GAN Fingerprints</a> is accepted to ICCV 2021 as <strong style="color: red;">Oral</strong>. The <a href="https://github.com/ningyu1991/ArtificialGANFingerprints" target="_blank" rel="nofollow">code</a> is released.
            <br>
            [2021/07] <a href="https://arxiv.org/pdf/2103.16748.pdf" target="_blank" rel="nofollow">Dual Contrastive Loss and Attention for GANs</a> is accepted to ICCV 2021.
            <br>
            [2021/05] <a href="https://www.sciencedirect.com/science/article/abs/pii/S0963996921003367" target="_blank" rel="nofollow">CV for Food Nutrient Prediction</a> is accepted to Food Research International 2021.
            <br>
            [2021/04] <a href="https://arxiv.org/pdf/2105.14376.pdf" target="_blank" rel="nofollow">Re-Synthesis for Deepfake Detection</a> is accepted to IJCAI 2021. The <a href="https://github.com/SSAW14/BeyondtheSpectrum" target="_blank" rel="nofollow">code</a> is released.
            <br>
            [2021/03] <a href="https://www.sciencedirect.com/science/article/abs/pii/S0889157521000570" target="_blank" rel="nofollow">AI for Food Nutrient Estimation</a> is accepted to Journal of Food Composition and Analysis 2021.
            <br>
            [2021/02] <a href="https://arxiv.org/pdf/2011.14107.pdf" target="_blank" rel="nofollow">Hijack-GAN</a> is accepted to CVPR 2021. The <a href="https://github.com/a514514772/hijackgan" target="_blank" rel="nofollow">code</a> is released.
            <br>
            [2020/08] <a href="https://arxiv.org/pdf/2004.03706.pdf" target="_blank" rel="nofollow">Class Balanced Experts for Long-Tailed Recognition</a> is accepted to GCPR 2020. The <a href="https://github.com/ssfootball04/class-balanced-experts" target="_blank" rel="nofollow">code</a> is released.
            <br>
            [2020/06] <a href="https://arxiv.org/pdf/2004.03355.pdf" target="_blank" rel="nofollow">Inclusive GAN</a> is accepted to ECCV 2020. The <a href="https://github.com/ningyu1991/InclusiveGAN" target="_blank" rel="nofollow">code</a> is released.
            <br>
            [2020/03] <a href="https://arxiv.org/pdf/1909.03935.pdf" target="_blank" rel="nofollow">GAN-Leaks</a> is accepted to CCS 2020. The <a href="https://github.com/DingfanChen/GAN-Leaks" target="_blank" rel="nofollow">code</a> is released.
        </p>
    </div>

    <div class="container">
        <p id="sect-publications">
            <h2>Publications</h2>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_UMDFood.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2306.01747.pdf" target="_blank" rel="nofollow">UMDFood: Vision-language models boost food composition compilation</a>
                            </strong>
                            <br>
                            <br>
                            Peihua Ma*, Yixin Wu*, <b><u>Ning Yu</u></b>, Yang Zhang, Michael Backes, Qin Wang, Cheng-I Wei
                            <br>
                            <br>
                            <b><em>arXiv 2023</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2306.01747.pdf" target="_blank" rel="nofollow">pdf</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_UniControl.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://canqin001.github.io/UniControl-Page/" target="_blank" rel="nofollow">UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild</a>
                            </strong>
                            <br>
                            <br>
                            Can Qin, Shu Zhang, <b><u>Ning Yu</u></b>, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Carlos Niebles, Caiming Xiong, Silvio Savarese, Stefano Ermon, Yun Fu, Ran Xu
                            <br>
                            <br>
                            <b><em>arXiv 2023</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2305.11147.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://canqin001.github.io/UniControl-Page/" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/salesforce/UniControl" target="_blank" rel="nofollow">code</a>
                                <a href="https://mp.weixin.qq.com/s/F8HXnkYhIbHZ-eSz9FsFEQ" target="_blank" rel="nofollow">press</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_ULIP-2.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2305.08275.pdf" target="_blank" rel="nofollow">ULIP-2: Towards Scalable Multimodal Pre-training For 3D Understanding</a>
                            </strong>
                            <br>
                            <br>
                            Le Xue, <b><u>Ning Yu</u></b>, Shu Zhang, Junnan Li, Roberto Martín-Martín, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, Silvio Savarese
                            <br>
                            <br>
                            <b><em>arXiv 2023</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2305.08275.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/salesforce/ULIP" target="_blank" rel="nofollow">code</a>
                                <a href="https://blog.salesforceairesearch.com/ulip/" target="_blank" rel="nofollow">blog</a>
                                <a href="https://mp.weixin.qq.com/s/eckq5-0b-A34WnjYZaQhcg" target="_blank" rel="nofollow">press</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_LayoutDETR.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="./projects/LayoutDETR.html" target="_blank" rel="nofollow">LayoutDETR: Detection Transformer Is a Good Multimodal Layout Designer</a>
                            </strong>
                            <br>
                            <br>
                            <b><u>Ning Yu</u></b>, Chia-Chih Chen, Zeyuan Chen, Rui Meng, Gang Wu, Paul Josel, Juan Carlos Niebles, Caiming Xiong, Ran Xu
                            <br>
                            <br>
                            <b><em>arXiv 2023</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2212.09877.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="./projects/LayoutDETR.html" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/salesforce/LayoutDETR" target="_blank" rel="nofollow">code</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_GlueGen.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://canqin001.github.io/GlueGen-Page" target="_blank" rel="nofollow">GlueGen: Plug and Play Multi-modal Encoders for X-to-image Generation</a>
                            </strong>
                            <br>
                            <br>
                            Can Qin, <b><u>Ning Yu</u></b>, Chen Xing, Shu Zhang, Zeyuan Chen, Stefano Ermon, Yun Fu, Caiming Xiong, Ran Xu
                            <br>
                            <br>
                            <b><em>arXiv 2023</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2303.10056.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://canqin001.github.io/GlueGen-Page" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/salesforce/GlueGen" target="_blank" rel="nofollow">code</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_HIVE.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://shugerdou.github.io/hive/" target="_blank" rel="nofollow">HIVE: Harnessing Human Feedback for Instructional Visual Editing</a>
                            </strong>
                            <br>
                            <br>
                            Shu Zhang*, Xinyi Yang*, Yihao Feng*, Can Qin, Chia-Chih Chen, <b><u>Ning Yu</u></b>, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, Caiming Xiong, Ran Xu
                            <br>
                            <br>
                            <b><em>arXiv 2023</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2303.09618.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://shugerdou.github.io/hive/" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/salesforce/HIVE" target="_blank" rel="nofollow">code</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_3dTransformer.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2301.02650.pdf" target="_blank" rel="nofollow">Model-Agnostic Hierarchical Attention for 3D Object Detection</a>
                            </strong>
                            <br>
                            <br>
                            Manli Shu, Le Xue, <b><u>Ning Yu</u></b>, Roberto Martín-Martín, Juan Carlos Niebles, Caiming Xiong, Ran Xu
                            <br>
                            <br>
                            <b><em>arXiv 2023</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2301.02650.pdf" target="_blank" rel="nofollow">pdf</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_DEFAKE.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2210.06998.pdf" target="_blank" rel="nofollow">DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image Diffusion Models</a>
                            </strong>
                            <br>
                            <br>
                            Zeyang Sha, Zheng Li, <b><u>Ning Yu</u></b>, Yang Zhang
                            <br>
                            <br>
                            <b><em>CCS 2023</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2210.06998.pdf" target="_blank" rel="nofollow">pdf</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_GGD.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2306.07758.pdf" target="_blank" rel="nofollow">Generated Graph Detection</a>
                            </strong>
                            <br>
                            <br>
                            Yihan Ma, Zhikun Zhang, <b><u>Ning Yu</u></b>, Xinlei He, Michael Backes, Yun Shen, Yang Zhang
                            <br>
                            <br>
                            <b><em>ICML 2023</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2306.07758.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/Yvonnemamama/GGD" target="_blank" rel="nofollow">code</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_SPAD.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://github.com/cc13qq/SAPD" target="_blank" rel="nofollow">Detecting Adversarial Faces Using Only Real Face Self-Perturbations</a>
                            </strong>
                            <br>
                            <br>
                            Qian Wang, Yongqin Xian, Hefei Ling, Jinyuan Zhang, XiaoRui Lin, Ping Li, Jiazhong Chen, <b><u>Ning Yu</u></b>
                            <br>
                            <br>
                            <b><em>IJCAI 2023</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2304.11359.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/cc13qq/SAPD" target="_blank" rel="nofollow">code</a>
                                <a href="https://www.youtube.com/watch?v=BZ5WSeSBeQE" target="_blank" rel="nofollow">video</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_PrototypeClassifiers.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2302.00491.pdf" target="_blank" rel="nofollow">Learning Prototype Classifiers for Long-Tailed Recognition</a>
                            </strong>
                            <br>
                            <br>
                            Saurabh Sharma, Yongqin Xian, <b><u>Ning Yu</u></b>, Ambuj Singh
                            <br>
                            <br>
                            <b><em>IJCAI 2023</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2302.00491.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/saurabhsharma1993/prototype-classifier-ltr" target="_blank" rel="nofollow">code</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_MaskFreeOVIS.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://vibashan.github.io/mask-free-ovis-web/" target="_blank" rel="nofollow">Mask-free OVIS: Open-Vocabulary Instance Segmentation without  Manual Mask Annotations</a>
                            </strong>
                            <br>
                            <br>
                            Vibashan VS, <b><u>Ning Yu</u></b>, Chen Xing, Can Qin, Mingfei Gao, Juan Carlos Niebles, Vishal Patel, Ran Xu
                            <br>
                            <br>
                            <b><em>CVPR 2023</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2303.16891.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://vibashan.github.io/mask-free-ovis-web/" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/Vibashan/Mask-free-OVIS" target="_blank" rel="nofollow">code</a>
                                <a href="./homepage_files/poster_MaskFreeOVIS.pdf" target="_blank" rel="nofollow">poster</a>
                                <a href="https://blog.salesforceairesearch.com/mask-free-ovis/" target="_blank" rel="nofollow">blog</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_ContSteal.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2201.07513.pdf" target="_blank" rel="nofollow">Can’t Steal? Cont-Steal! Contrastive Stealing Attacks Against Image Encoders</a>
                            </strong>
                            <br>
                            <br>
                            Zeyang Sha, Xinlei He, <b><u>Ning Yu</u></b>, Michael Backes, Yang Zhang
                            <br>
                            <br>
                            <b><em>CVPR 2023</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2201.07513.pdf" target="_blank" rel="nofollow">pdf</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_RoSteALS.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://github.com/TuBui/RoSteALS" target="_blank" rel="nofollow">RoSteALS: Robust Steganography using Autoencoder Latent Space</a>
                            </strong>
                            <br>
                            <br>
                            Tu Bui, Shruti Agarwal, <b><u>Ning Yu</u></b>, John Collomosse
                            <br>
                            <br>
                            <b><em>CVPR Workshop on Media Forensics 2023</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2304.03400.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/TuBui/RoSteALS" target="_blank" rel="nofollow">code</a>
                                <a href="https://huggingface.co/spaces/tubui/rosteal">Huggingface space</a>
                                <a href="./homepage_files/poster_RoSteALS.pdf">poster</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_UnGANable.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2210.00957.pdf" target="_blank" rel="nofollow">UnGANable: Defending Against GAN-based Face Manipulation</a>
                            </strong>
                            <br>
                            <br>
                            Zheng Li, <b><u>Ning Yu</u></b>, Ahmed Salem, Michael Backes, Mario Fritz, Yang Zhang
                            <br>
                            <br>
                            <b><em>USENIX 2023</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2210.00957.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/zhenglisec/UnGANable" target="_blank" rel="nofollow">code</a>
                                <a href="./homepage_files/poster_UnGANable.pdf" target="_blank" rel="nofollow">poster</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_FcFInpainting.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://praeclarumjj3.github.io/fcf-inpainting/" target="_blank" rel="nofollow">Keys to Better Image Inpainting: Structure and Texture Go Hand in Hand</a>
                            </strong>
                            <br>
                            <br>
                            Jitesh Jain*, Yuqian Zhou*, <b><u>Ning Yu</u></b>, Humphrey Shi
                            <br>
                            <br>
                            <b><em>WACV 2023</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2208.03382.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://praeclarumjj3.github.io/fcf-inpainting/" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/SHI-Labs/FcF-Inpainting" target="_blank" rel="nofollow">code</a>
                                <a href="https://colab.research.google.com/github/SHI-Labs/FcF-Inpainting/blob/main/colab/FcF_Inpainting.ipynb" target="_blank" rel="nofollow">demo</a>
                                <a href="https://huggingface.co/spaces/shi-lab/FcF-Inpainting">Huggingface space</a>
                                <a href="./homepage_files/poster_FcF-Inpainting.pdf">poster</a>
                                <a href="https://docs.google.com/presentation/d/1TPvnzGBq5G45r_-BRzLUdopjeq_KGUkQPGWDKAdaws0/edit?usp=share_link">slides</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_Augtriever.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2212.08841.pdf" target="_blank" rel="nofollow">Unsupervised Dense Retrieval Deserves Better Positive Pairs: Scalable Augmentation with Query Extraction and Generation</a>
                            </strong>
                            <br>
                            <br>
                            Rui Meng, Ye Liu, Semih Yavuz, Divyansh Agarwal, Lifu Tu, <b><u>Ning Yu</u></b>, Jianguo Zhang, Meghana Bhat, Yingbo Zhou
                            <br>
                            <br>
                            <b><em>arXiv 2022</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2212.08841.pdf" target="_blank" rel="nofollow">pdf</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_SimSCOOD.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2210.04802.pdf" target="_blank" rel="nofollow">SimSCOOD: Systematic Analysis of Out-of-Distribution Behavior of Source Code Models</a>
                            </strong>
                            <br>
                            <br>
                            Hossein Hajipour, <b><u>Ning Yu</u></b>, Cristian-Alexandru Staicu, Mario Fritz
                            <br>
                            <br>
                            <b><em>arXiv 2022</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2210.04802.pdf" target="_blank" rel="nofollow">pdf</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_ArtistsMIA.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2210.00968.pdf" target="_blank" rel="nofollow">Membership Inference Attacks Against Text-to-image Generation Models</a>
                            </strong>
                            <br>
                            <br>
                            Yixin Wu, <b><u>Ning Yu</u></b>, Zheng Li, Michael Backes, Yang Zhang
                            <br>
                            <br>
                            <b><em>arXiv 2022</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2210.00968.pdf" target="_blank" rel="nofollow">pdf</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_MultiExitPrivacy.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2208.11180.pdf" target="_blank" rel="nofollow">Auditing Membership Leakages of Multi-Exit Networks</a>
                            </strong>
                            <br>
                            <br>
                            Zheng Li, Yiyong Liu, Xinlei He, <b><u>Ning Yu</u></b>, Michael Backes, Yang Zhang
                            <br>
                            <br>
                            <b><em>CCS 2022</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2208.11180.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/zhenglisec/Multi-Exit-Privacy" target="_blank" rel="nofollow">code</a>
                                <a href="./homepage_files/slides_MultiExitPrivacy.pptx" target="_blank" rel="nofollow">slides</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_RepMix.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2207.02063.pdf" target="_blank" rel="nofollow">RepMix: Representation Mixing for Robust Attribution of Synthesized Images</a>
                            </strong>
                            <br>
                            <br>
                            Tu Bui, <b><u>Ning Yu</u></b>, John Collomosse
                            <br>
                            <br>
                            <b><em>ECCV 2022</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2207.02063.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/TuBui/image_attribution" target="_blank" rel="nofollow">code</a>
                                <a href="./homepage_files/poster_RepMix.pdf" target="_blank" rel="nofollow">poster</a>
                                <a href="https://kahlan.cvssp.org/data/Flickr25K/tubui/eccv22_repmix/5993.mp4" target="_blank" rel="nofollow">video</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_FoodNutrientPrediction2.jpg" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://www.sciencedirect.com/science/article/pii/S0308814622012055" target="_blank" rel="nofollow">Deep Learning Accurately Predicts Food Categories and Nutrients Based on Ingredient Statements</a>
                            </strong>
                            <br>
                            <br>
                            Peihua Ma, Zhikun Zhang, Ying Li,  <b><u>Ning Yu</u></b>, Jiping Sheng, Hande Küçük McGinty, Qin Wang, Jaspreet Ahuja
                            <br>
                            <br>
                            <b><em>Food Chemistry 2022</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://www.sciencedirect.com/science/article/pii/S0308814622012055" target="_blank" rel="nofollow">pdf</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_ScalableGANFingerprints.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="./projects/ScalableGANFingerprints.html" target="_blank" rel="nofollow">Responsible Disclosure of Generative Models Using Scalable Fingerprinting</a>
                            </strong>
                            <br>
                            <br>
                            <b><u>Ning Yu</u></b>*, Vladislav Skripniuk*, Dingfan Chen, Larry Davis, Mario Fritz
                            <br>
                            <br>
                            <b><em>ICLR 2022 <strong style="color: red;">Spotlight</strong></em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2012.08726.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="./projects/ScalableGANFingerprints.html" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/ningyu1991/ScalableGANFingerprints" target="_blank" rel="nofollow">code</a>
                                <a href="./homepage_files/poster_ScalableGANFingerprints.pdf" target="_blank" rel="nofollow">poster</a>
                                <a href="https://www.youtube.com/watch?v=UlpGtwEof3o" target="_blank" rel="nofollow">video</a>
                                <a href="https://mp.weixin.qq.com/s/v8hQT8VMjxe0zHybhxu2Aw" target="_blank" rel="nofollow">press</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_RelaxLoss.jpeg" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://openreview.net/pdf?id=FEDfGWVZYIn" target="_blank" rel="nofollow">RelaxLoss: Defending Membership Inference Attacks without Losing Utility</a>
                            </strong>
                            <br>
                            <br>
                            Dingfan Chen, <b><u>Ning Yu</u></b>, Mario Fritz
                            <br>
                            <br>
                            <b><em>ICLR 2022 <strong style="color: red;">Spotlight</strong></em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://openreview.net/pdf?id=FEDfGWVZYIn" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/DingfanChen/RelaxLoss" target="_blank" rel="nofollow">code</a>
                                <a href="./homepage_files/poster_RelaxLoss.pdf" target="_blank" rel="nofollow">poster</a>
                                <a href="https://www.youtube.com/watch?v=Iyu0gNC3oYE&t=5s" target="_blank" rel="nofollow">video</a>
                                <a href="https://mp.weixin.qq.com/s/Z-wAHDg7dLCy4NQMJzXXPQ" target="_blank" rel="nofollow">press</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_HumanCentricDeepGenerativeModels.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://drum.lib.umd.edu/handle/1903/27856" target="_blank" rel="nofollow">Human-Centric Deep Generative Models: The Blessing and The Curse</a>
                            </strong>
                            <br>
                            <br>
                            <b><u>Ning Yu</u></b>
                            <br>
                            <br>
                            <b><em>Ph.D. Dissertation 2021, University of Maryland and Max Planck Institute for Informatics</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://drum.lib.umd.edu/handle/1903/27856" target="_blank" rel="nofollow">pdf</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_VIDNet.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2101.11080.pdf" target="_blank" rel="nofollow">Deep Video Inpainting Detection</a>
                            </strong>
                            <br>
                            <br>
                            Peng Zhou, <b><u>Ning Yu</u></b>, Zuxuan Wu, Larry Davis, Abhinav Shrivastava, Ser-Nam Lim
                            <br>
                            <br>
                            <b><em>BMVC 2021</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2101.11080.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/pengzhou1108/VIDNet" target="_blank" rel="nofollow">code</a>
                                <a href="https://www.bmvc2021-virtualconference.com/conference/papers/paper_0398.html" target="_blank" rel="nofollow">video</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_ArtificialGANFingerprints.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="./projects/ArtificialGANFingerprints.html" target="_blank" rel="nofollow">Artificial Fingerprinting for Generative Models: Rooting Deepfake Attribution in Training Data</a>
                            </strong>
                            <br>
                            <br>
                            <b><u>Ning Yu</u></b>*, Vladislav Skripniuk*, Sahar Abdelnabi, Mario Fritz
                            <br>
                            <br>
                            <b><em>ICCV 2021 <strong style="color: red;">Oral</strong></em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2007.08457.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="./projects/ArtificialGANFingerprints.html" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/ningyu1991/ArtificialGANFingerprints" target="_blank" rel="nofollow">code</a>
                                <a href="./homepage_files/poster_ArtificialGANFingerprints.pdf" target="_blank" rel="nofollow">poster</a>
                                <a href="https://www.youtube.com/watch?v=j8bcOHhu4Lg" target="_blank" rel="nofollow">video</a>
                                <a href="https://mp.weixin.qq.com/s/PDxLWP7CYpDcD5y3aGQ7Rg" target="_blank" rel="nofollow">press</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_AttentionContrastGAN.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="./projects/AttentionDualContrastGAN.html" target="_blank" rel="nofollow">Dual Contrastive Loss and Attention for GANs</a>
                            </strong>
                            <br>
                            <br>
                            <b><u>Ning Yu</u></b>, Guilin Liu, Aysegul Dundar, Andrew Tao, Bryan Catanzaro, Larry Davis, Mario Fritz
                            <br>
                            <br>
                            <b><em>ICCV 2021</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2103.16748.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="./projects/AttentionDualContrastGAN.html" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/ningyu1991/AttentionDualContrastGAN" target="_blank" rel="nofollow">code</a>
                                <a href="./homepage_files/poster_AttentionDualContrastGAN.pdf" target="_blank" rel="nofollow">poster</a>
                                <a href="https://www.youtube.com/watch?v=hviCTQJzhd0" target="_blank" rel="nofollow">video</a>
                                <a href="https://mp.weixin.qq.com/s/vqc97WuwEMEYNuCJRQJE5A" target="_blank" rel="nofollow">press</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_ResynthesizerGANDetection.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://ssaw14.github.io/BeyondtheSpectrum/" target="_blank" rel="nofollow">Beyond the Spectrum: Detecting Deepfakes via Re-Synthesis</a>
                            </strong>
                            <br>
                            <br>
                            Yang He, <b><u>Ning Yu</u></b>, Margret Keuper, Mario Fritz
                            <br>
                            <br>
                            <b><em>IJCAI 2021</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2105.14376.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://ssaw14.github.io/BeyondtheSpectrum/" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/SSAW14/BeyondtheSpectrum" target="_blank" rel="nofollow">code</a>
                                <a href="https://www.youtube.com/watch?v=kQeREkzrrPM" target="_blank" rel="nofollow">video</a>
                                <a href="https://mp.weixin.qq.com/s/Vffg2wexqcEpJYDhXg__7g" target="_blank" rel="nofollow">press</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_HijackGAN.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://a514514772.github.io/hijackgan/" target="_blank" rel="nofollow">Hijack-GAN: Unintended-Use of Pretrained, Black-Box GANs</a>
                            </strong>
                            <br>
                            <br>
                            Hui-Po Wang, <b><u>Ning Yu</u></b>, Mario Fritz
                            <br>
                            <br>
                            <b><em>CVPR 2021</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2011.14107.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://a514514772.github.io/hijackgan/" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/a514514772/hijackgan" target="_blank" rel="nofollow">code</a>
                                <a href="https://a514514772.github.io/hijackgan/assets/poster.pdf" target="_blank" rel="nofollow">poster</a>
                                <a href="https://www.youtube.com/watch?v=M3rv4fqUoNQ" target="_blank" rel="nofollow">video</a>
                                <a href="https://mp.weixin.qq.com/s/Z7YGrGYxvV12AUt6sJUvJg" target="_blank" rel="nofollow">press</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_ChineseMarketFoodNutrientEstimation.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://www.sciencedirect.com/science/article/abs/pii/S0308814621020008" target="_blank" rel="nofollow">Application of Deep Learning for Image-based Chinese Market Food Nutrients Estimation</a>
                            </strong>
                            <br>
                            <br>
                            Peihua Ma, Chun Pong Lau, <b><u>Ning Yu</u></b>, An Li, Jiping Sheng
                            <br>
                            <br>
                            <b><em>Food Chemistry 2021</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://www.sciencedirect.com/science/article/abs/pii/S0308814621020008" target="_blank" rel="nofollow">pdf</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_ChineseFoodNutrientPrediction.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://www.sciencedirect.com/science/article/abs/pii/S0963996921003367" target="_blank" rel="nofollow">Image-based Nutrient Estimation for Chinese Dishes Using Deep Learning</a>
                            </strong>
                            <br>
                            <br>
                            Peihua Ma, Chun Pong Lau, <b><u>Ning Yu</u></b>, An Li, Ping Liu, Qin Wang, Jiping Sheng
                            <br>
                            <br>
                            <b><em>Food Research International 2021</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://www.sciencedirect.com/science/article/abs/pii/S0963996921003367" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://mp.weixin.qq.com/s/RY-PVK7ogCHbVxf5CpMStg" target="_blank" rel="nofollow">press</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_FoodNutrientPrediction.jpg" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://www.sciencedirect.com/science/article/abs/pii/S0889157521000570" target="_blank" rel="nofollow">Application of Machine Learning for Predicting Label Nutrients Using USDA Global Branded Food Products Database (BFPD)</a>
                            </strong>
                            <br>
                            <br>
                            Peihua Ma, An Li, <b><u>Ning Yu</u></b>, Ying Li, Rahul Bahadur, Qin Wang, Jaspreet Ahuja
                            <br>
                            <br>
                            <b><em>Journal of Food Composition and Analysis 2021</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://www.sciencedirect.com/science/article/abs/pii/S0889157521000570" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://mp.weixin.qq.com/s/Q_Gq1R5pRuxQQGvvrYnn_Q" target="_blank" rel="nofollow">press</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_ClassBalancedExperts.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2004.03706.pdf" target="_blank" rel="nofollow">Long-Tailed Recognition Using Class-Balanced Experts</a>
                            </strong>
                            <br>
                            <br>
                            Saurabh Sharma, <b><u>Ning Yu</u></b>, Mario Fritz, Bernt Schiele
                            <br>
                            <br>
                            <b><em>GCPR 2020</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2004.03706.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/ssfootball04/class-balanced-experts" target="_blank" rel="nofollow">code</a>
                                <a href="https://www.youtube.com/watch?v=1rxMDoIm6oM&feature=youtu.be&t=29m58s" target="_blank" rel="nofollow">video</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_GANLeaks.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/1909.03935.pdf" target="_blank" rel="nofollow">GAN-Leaks: A Taxonomy of Membership Inference Attacks against GANs</a>
                            </strong>
                            <br>
                            <br>
                            Dingfan Chen, <b><u>Ning Yu</u></b>, Yang Zhang, Mario Fritz
                            <br>
                            <br>
                            <b><em>CCS 2020</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/1909.03935.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/DingfanChen/GAN-Leaks" target="_blank" rel="nofollow">code</a>
                                <a href="./homepage_files/poster_GANLeaks.pdf" target="_blank" rel="nofollow">poster</a>
                                <a href="https://www.youtube.com/watch?v=UkOe_sGRYec" target="_blank" rel="nofollow">video (short)</a>
                                <a href="https://www.youtube.com/watch?v=APnsV8AFXZQ" target="_blank" rel="nofollow">video (full)</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_InclusiveGAN.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="./projects/InclusiveGAN.html" target="_blank" rel="nofollow">Inclusive GAN: Improving Data and Minority Coverage in Generative Models</a>
                            </strong>
                            <br>
                            <br>
                            <b><u>Ning Yu</u></b>, Ke Li, Peng Zhou, Jitendra Malik, Larry Davis, Mario Fritz
                            <br>
                            <br>
                            <b><em>ECCV 2020</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2004.03355.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="./projects/InclusiveGAN.html" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/ningyu1991/InclusiveGAN" target="_blank" rel="nofollow">code</a>
                                <a href="https://www.youtube.com/watch?v=JbHWuLsn_zg" target="_blank" rel="nofollow">video (short)</a>
                                <a href="https://www.youtube.com/watch?v=oCb4cpsQ7do" target="_blank" rel="nofollow">video (full)</a>
                                <a href="https://mp.weixin.qq.com/s/6CCWQY8d0NoHEuMqWEp2dw" target="_blank" rel="nofollow">press</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_GANFingerprints.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="./projects/GANFingerprints.html" target="_blank" rel="nofollow">Attributing Fake Images to GANs: Learning and Analyzing GAN Fingerprints</a>
                            </strong>
                            <br>
                            <br>
                            <b><u>Ning Yu</u></b>, Larry Davis, Mario Fritz
                            <br>
                            <br>
                            <b><em>ICCV 2019</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/1811.08180.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="./projects/GANFingerprints.html" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/ningyu1991/GANFingerprints" target="_blank" rel="nofollow">code</a>
                                <a href="./homepage_files/poster_GANFingerprints.pdf" target="_blank" rel="nofollow">poster</a>
                                <a href="https://mp.weixin.qq.com/s/se1ZyR_gfzliWB5X72OZ1Q" target="_blank" rel="nofollow">press</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_TextureMixer.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="./projects/TextureMixer.html" target="_blank" rel="nofollow">Texture Mixer: A Network for Controllable Synthesis and Interpolation of Texture</a>
                            </strong>
                            <br>
                            <br>
                            <b><u>Ning Yu</u></b>, Connelly Barnes, Eli Shechtman, Sohrab Amirghodsi, Michal Lukáč
                            <br>
                            <br>
                            <b><em>CVPR 2019</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/1901.03447.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="./projects/TextureMixer.html" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/ningyu1991/TextureMixer" target="_blank" rel="nofollow">code</a>
                                <a href="./homepage_files/poster_TextureMixer.pdf" target="_blank" rel="nofollow">poster</a>
                                <a href="https://mp.weixin.qq.com/s/b_gIB_zpUCcf2a8ImKMXRA" target="_blank" rel="nofollow">press</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_DefectDetection.jpg" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="./projects/DefectDetection.html" target="_blank" rel="nofollow">Learning to Detect Multiple Photographic Defects</a>
                            </strong>
                            <br>
                            <br>
                            <b><u>Ning Yu</u></b>, Xiaohui Shen, Zhe Lin, Radomír Měch, Connelly Barnes
                            <br>
                            <br>
                            <b><em>WACV 2018</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/1612.01635.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="./homepage_files/supp_DefectDetection.pdf" target="_blank" rel="nofollow">supp</a>
                                <a href="./projects/DefectDetection.html" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/ningyu1991/DefectDetection" target="_blank" rel="nofollow">code</a>
                                <a href="./homepage_files/poster_DefectDetection.pdf" target="_blank" rel="nofollow">poster</a>
                                <a href="./homepage_files/slides_DefectDetection.pptx" target="_blank" rel="nofollow">slides</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_SupervoxelMRFSeg.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="./projects/SupervoxelMRFSeg.html" target="_blank" rel="nofollow">Supervoxel-Based Hierarchical Markov Random Field Framework for Multi-Atlas Segmentation</a>
                            </strong>
                            <br>
                            <br>
                            <b><u>Ning Yu</u></b>, Hongzhi Wang, Paul Yushkevich
                            <br>
                            <br>
                            <b><em>MICCAI Workshop 2016</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="./homepage_files/paper_SupervoxelMRFSeg.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="./projects/SupervoxelMRFSeg.html" target="_blank" rel="nofollow">project</a>
                                <a href="./homepage_files/slides_SupervoxelMRFSeg.pptx" target="_blank" rel="nofollow">slides</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_SuperpixelBreastTumorSeg.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="./projects/SuperpixelBreastTumorSeg.html" target="_blank" rel="nofollow">A Superpixel-Based Framework for Automatic Tumor Segmentation on Breast DCE-MRI</a>
                            </strong>
                            <br>
                            <br>
                            <b><u>Ning Yu</u></b>, Jia Wu, Susan Weinstein, Bilwaj Gaonkar, Brad Keller, Ahmed Ashraf, YunQing Jiang, Christos Davatzikos, Emily Conant, Despina Kontos
                            <br>
                            <br>
                            <b><em>SPIE Medical Imaging 2015 <strong style="color: red;">Oral</strong>, <strong style="color: red;">best student paper finalist</strong></em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="./homepage_files/paper_SuperpixelBreastTumorSeg.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="./projects/SuperpixelBreastTumorSeg.html" target="_blank" rel="nofollow">project</a>
                                <a href="./homepage_files/slides_SuperpixelBreastTumorSeg.pptx" target="_blank" rel="nofollow">slides</a>
                                <a href="https://web.archive.org/web/20210422170343/http://spie.org/about-spie/press-room/mi15-news" target="_blank" rel="nofollow">press</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_QuantBreastTumorReg.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="./homepage_files/paper_QuantBreastTumorReg.pdf" target="_blank" rel="nofollow">Quantification of Tumor Changes during Neoadjuvant Chemotherapy with Longitudinal Breast DCE-MRI Registration</a>
                            </strong>
                            <br>
                            <br>
                            Jia Wu, Yangming Ou, Susan Weinstein, Emily Conant, <b><u>Ning Yu</u></b>, Vahid Hoshmand, Brad Keller, Ahmed Ashraf, Mark Rosen, Angela DeMichele, Christos Davatzikos,  Despina Kontos
                            <br>
                            <br>
                            <b><em>SPIE Medical Imaging 2015</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="./homepage_files/paper_QuantBreastTumorReg.pdf" target="_blank" rel="nofollow">pdf</a>
    、                        </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_BreastTumorPattern.jpg" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="./homepage_files/paper_BreastTumorPattern.pdf" target="_blank" rel="nofollow">Tumor Heterogeneity Patterns of DCE-MRI Parametric Response Maps May Augment Early Assessment of Neoadjuvant Chemotherapy: A Pilot Study of ACRIN 6657/I-SPY 1</a>
                            </strong>
                            <br>
                            <br>
                            Jia Wu, Susan Weinstein, Andrew Oustimov, Lauren Pantalone, <b><u>Ning Yu</u></b>, Yangming Ou, Mark Rosen, Angela DeMichele, Christos Davatzikos, Despina Kontos
                            <br>
                            <br>
                            <b><em>RSNA 2015</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="./homepage_files/paper_BreastTumorPattern.pdf" target="_blank" rel="nofollow">pdf</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_InvestigationBreatTumorReg.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="./homepage_files/paper_InvestigationBreatTumorReg.pdf" target="_blank" rel="nofollow">A Feasibility Study Investigating the Use of Quantitative Measures of Spatio-Temporal Tumor Heterogeneity Derived from 4D Breast DCE-MRI Registration as a Biomarker of Response to Neoadjuvant Chemotherapy</a>
                            </strong>
                            <br>
                            <br>
                            Jia Wu, Yangming Ou, Susan Weinstein, Emily Conant, <b><u>Ning Yu</u></b>, Vahid Hoshmand, Brad Keller, Ahmed Ashraf, Mark Rosen, Angela DeMichele, Christos Davatzikos,  Despina Kontos
                            <br>
                            <br>
                            <b><em>ISMRM Workshop 2014</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="./homepage_files/paper_InvestigationBreatTumorReg.pdf" target="_blank" rel="nofollow">pdf</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_FeaturePointCorrespondencesTracking.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="./projects/FeaturePointCorrespondencesTracking.html" target="_blank" rel="nofollow">Robust Feature Points Correspondences for Visual Object Tracking</a>
                            </strong>
                            <br>
                            <br>
                            <b><u>Ning Yu</u></b>
                            <br>
                            <br>
                            <b><em>Undergraduate Thesis 2013, Huazhong University of Science and Technology</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="./homepage_files/paper_FeaturePointCorrespondencesTracking.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="./projects/FeaturePointCorrespondencesTracking.html" target="_blank" rel="nofollow">project</a>
                                <a href="./homepage_files/slides_FeaturePointCorrespondencesTracking.pptx" target="_blank" rel="nofollow">slides</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
        </p>

    <div class="container">
        <p id="sect-patents">
            <h2>Patents</h2>
            <br>
            <div class="publication">
                <strong>
                    Systems and Methods for Unsupervised Training in Text Retrieval Tasks
                </strong>
                <br>
                Rui Meng, Yingbo Zhou, Ye Liu, Semih Yavuz, <b><u>Ning Yu</u></b>
                <br>
                <em>US Application No. 18,303,313</em>
            </div>
            <br>
            <div class="publication">
                <strong>
                    Systems and Methods for Multimodal Layout Designs of Digital Publications
                </strong>
                <br>
                <b><u>Ning Yu</u></b>, Chia-Chih Chen, Zeyuan Chen, Caiming Xiong, Ran Xu, Juan Carlos Niebles, Rui Meng
                <br>
                <em>US Application No. 18,161,680</em>
            </div>
            <br>
            <div class="publication">
                <strong>
                    Systems and Methods for Text-to-Image Generation Using Language Models
                </strong>
                <br>
                <b><u>Ning Yu</u></b>, Can Qin, Chen Xing, Shu Zhang, Stefano Ermon, Caiming Xiong, Ran Xu
                <br>
                <em>US Application No. 18,162,535</em>
            </div>
            <br>
            <div class="publication">
                <strong>
                    Systems and Methods for Open-Vocabulary Instance Segmentation
                </strong>
                <br>
                <b><u>Ning Yu</u></b>, Vibashan VS, Chen Xing, Juan Carlos Niebles, Ran Xu
                <br>
                <em>US Application No. 18,159,318</em>
            </div>
            <br>
            <div class="publication">
                <strong>
                    Systems and Methods for Attention Mechanism in Three-Dimensional Object Detection
                </strong>
                <br>
                Manli Shu, Le Xue, <b><u>Ning Yu</u></b>, Roberto Martín-Martín, Juan Carlos Niebles, Caiming Xiong, Ran Xu
                <br>
                <em>US Application No. 18,161,661</em>
            </div>
            <br>
            <div class="publication">
                <strong>
                    Neural Network Training Technique
                </strong>
                <br>
                Guilin Liu, <b><u>Ning Yu</u></b>, Aysegul Dundar, Andrew Tao, Bryan Catanzaro
                <br>
                <em>US Application No. 17,165,745</em>
            </div>
            <br>
            <div class="publication">
                <strong>
                    <a href="https://patentimages.storage.googleapis.com/43/18/50/b310bb01ca1ef7/US10818043.pdf" target="_blank" rel="nofollow">Texture Interpolation Using Neural Networks</a>
                </strong>
                <br>
                Connelly Barnes, Sohrab Amirghodsi, Michal Lukáč, Eli Shechtman, <b><u>Ning Yu</u></b>
                <br>
                <em>US Patent No. 10,818,043</em>
                <br>
                <span class="links">
                    <a href="https://patentimages.storage.googleapis.com/43/18/50/b310bb01ca1ef7/US10818043.pdf" target="_blank" rel="nofollow">pdf</a>
                </span>
            </div>
            <br>
            <div class="publication">
                <strong>
                    <a href="https://patentimages.storage.googleapis.com/fb/a4/32/3d3030419f6859/US10810721.pdf" target="_blank" rel="nofollow">Digital Image Defect Identification and Correction</a>
                </strong>
                <br>
                Radomír Měch, <b><u>Ning Yu</u></b>, Xiaohui Shen, Zhe Lin
                <br>
                <em>US Patent No. 10,810,721</em>
                <br>
                <span class="links">
                    <a href="https://patentimages.storage.googleapis.com/fb/a4/32/3d3030419f6859/US10810721.pdf" target="_blank" rel="nofollow">pdf</a>
                </span>
            </div>
        </div>
    </p>

    <div class="container">
        <p id="sect-awards">
            <h2>Selected Awards</h2>
            <br>
            <div>
                [2022] <a href="https://icml.cc/Conferences/2022/Reviewers" target="_blank" rel="nofollow">ICML'22 Top Reviewer</a>
                <br>
                [2021] <a href="https://blog.twitch.tv/en/2021/01/07/introducing-our-2021-twitch-research-fellows/" target="_blank" rel="nofollow">Twitch (Amazon) Research Fellowship</a>
                <br>
                [2020] <a href="https://www.qualcomm.com/invention/research/university-relations/innovation-fellowship/finalists" target="_blank" rel="nofollow">Qualcomm Innovation Fellowship Finalist</a>
                <br>
                [2019] <a href="https://www.qualcomm.com/invention/research/university-relations/innovation-fellowship/finalists" target="_blank" rel="nofollow">Qualcomm Innovation Fellowship Finalist</a>
                <br>
                [2015] <a href="https://web.archive.org/web/20210422170343/http://spie.org/about-spie/press-room/mi15-news" target="_blank" rel="nofollow">SPIE Best Student Paper Finalist</a>
                <br>
                [2015] <a href="https://web.archive.org/web/20210422170343/http://spie.org/about-spie/press-room/mi15-news" target="_blank" rel="nofollow">SPIE Travel Scholarship</a>
                <br>
                [2012] Chinese National Fellowship
                <br>
                [2012] Microsoft Young Fellowship
                <br>
                [2012] Meritorious Winner of the Mathematical Contest in Modeling
                <br>
                [2011] 2nd Prize of the Chinese Mathematical Contest in Modeling
            </div>
        </div>
    </p>

    <div class="container">
        <p id="sect-mentoring">
            <h2>Mentoring</h2>
            <br>
            <div>
                <a href="https://github.com/williamium3000" target="_blank" rel="nofollow">Yijiang Li</a> @JHU
                <br>
                <a href="https://www.linkedin.com/in/gong-zhang-a3820a77/" target="_blank" rel="nofollow">Gong Zhang</a> @GaTech
                <br>
                <a href="https://wangk.ai/" target="_blank" rel="nofollow">Kai Wang</a> @GaTech
                <br>
                <a href="https://sites.google.com/view/about-shiyuwang" target="_blank" rel="nofollow">Shiyu Wang</a> @Emory
                <br>
                <a href="https://artemisp.github.io/" target="_blank" rel="nofollow">Artemis Panagopoulou</a> @UPenn
                <br>
                <a href="" target="_blank" rel="nofollow">You-Ming Chang</a> @NCTU
                <br>
                <a href="" target="_blank" rel="nofollow">Chen Yeh</a> @NCTU
                <br>
                <a href="https://cc13qq.github.io/" target="_blank" rel="nofollow">Qian Wang</a> @HUST
                <br>
                <a href="https://minxingzhang.github.io/" target="_blank" rel="nofollow">Minxing Zhang</a> @CISPA
                <br>
                <a href="https://scholar.google.com/citations?hl=en&user=MYRUJkUAAAAJ&view_op=list_works&sortby=pubdate" target="_blank" rel="nofollow">Akash Gokul</a> @Berkeley
                <br>
                <a href="https://azshue.github.io/" target="_blank" rel="nofollow">Manli Shu</a> @UMD
                <br>
                <a href="https://vibashan.github.io/" target="_blank" rel="nofollow">Vibashan VS</a> @JHU
                <br>
                <a href="https://canqin.tech/" target="_blank" rel="nofollow">Can Qin</a> @NEU
                <br>
                <a href="https://applexy.github.io/cv/" target="_blank" rel="nofollow">Yuan Xin</a> @CISPA
                <br>
                <a href="https://yxoh.github.io/" target="_blank" rel="nofollow">Yixin Wu</a> @CISPA
                <br>
                <a href="https://www.linkedin.com/in/shuo-wen-7135a2208/?originalSubdomain=ch" target="_blank" rel="nofollow">Shuo Wen</a> @MPI-INF
                <br>
                <a href="https://fdszy.github.io/" target="_blank" rel="nofollow">Zeyang Sha</a> @CISPA
                <br>
                <a href="https://yvonnemamama.github.io/" target="_blank" rel="nofollow">Yihan Ma</a> @CISPA
                <br>
                <a href="https://praeclarumjj3.github.io/" target="_blank" rel="nofollow">Jitesh Jain</a> @IIT
                <br>
                <a href="https://zhenglisec.github.io/" target="_blank" rel="nofollow">Zheng Li</a> @CISPA
                <br>
                <a href="https://www.linkedin.com/in/vladislav-skripniuk-8a8891143/?originalSubdomain=ru" target="_blank" rel="nofollow">Vladislav Skripniuk</a> @Audatic
                <br>
                <a href="https://a514514772.github.io/#publications" target="_blank" rel="nofollow">Hui-Po Wang</a> @CISPA
                <br>
                <a href="https://dingfanchen.github.io/homepage/" target="_blank" rel="nofollow">Dingfan Chen</a> @CISPA
                <br>
                <a href="https://dynamo.cs.ucsb.edu/people/sharma" target="_blank" rel="nofollow">Saurabh Sharma</a> @UCSB
            </div>
        </div>
    </p>

    <div class="container">
        <p id="sect-chairing">
            <h2>Chairing</h2>
            <br>
            <div>
                <table><tbody>
                    <tr>
                        <td>CVPR area chair</td> <td>2023,2024</td>
                    </tr>
                    <tr>
                        <td>ICCV area chair</td> <td>2023</td>
                    </tr>
                    <tr>
                        <td>NeurIPS area chair</td> <td>2023</td>
                    </tr>
                    <tr>
                        <td>ICML session chair</td> <td>2022</td>
                    </tr>
                    <tr>
                        <td>AAAI senior program committee</td> <td>2023,2024</td>
                    </tr>
                    <tr>
                        <td>AISTATS area chair</td> <td>2023</td>
                    </tr>
                    <tr>
                        <td>BMVC area chair</td> <td>2022,2023</td>
                    </tr>
                    <tr>
                        <td>WACV area chair</td> <td>2023</td>
                    </tr>
                </tbody></table>
            </div>
        </div>
    </p>

    <div class="container">
        <p id="sect-reviewing">
            <h2>Reviewing</h2>
            <br>
            <div>
                <table><tbody>
                    <tr>
                        <td>CVPR</td> <td>since 2020</td>
                    </tr>
                    <tr>
                        <td>ICCV</td> <td>since 2019</td>
                    </tr>
                    <tr>
                        <td>ECCV</td> <td>since 2020</td>
                    </tr>
                    <tr>
                        <td>NeurIPS</td> <td>since 2021</td>
                    </tr>
                    <tr>
                        <td>ICML</td> <td>since 2021</td>
                    </tr>
                    <tr>
                        <td>ICLR</td> <td>since 2022</td>
                    </tr>
                    <tr>
                        <td>AAAI</td> <td>since 2020</td>
                    </tr>
                    <tr>
                        <td>IJCAI</td> <td>since 2020</td>
                    </tr>
                    <tr>
                        <td>SIGGRAPH</td> <td>since 2021</td>
                    </tr>
                    <tr>
                        <td>Eurographics</td> <td>since 2020</td>
                    </tr>
                    <tr>
                        <td>ICRA</td> <td>since 2020</td>
                    </tr>
                    <tr>
                        <td>TPAMI</td> <td>since 2019</td>
                    </tr>
                    <tr>
                        <td>IJCV</td> <td>since 2021</td>
                    </tr>
                    <tr>
                        <td>TOG</td> <td>since 2020</td>
                    </tr>
                    <tr>
                        <td>PR Letters</td> <td>since 2020</td>
                    </tr>
                    <tr>
                        <td>Neurocomputing</td> <td>since 2019</td>
                    </tr>
                    <tr>
                        <td>TVCJ</td> <td>since 2018</td>
                    </tr>
                </tbody></table>
            </div>
        </div>
    </p>

    <div class="container">
        <p id="sect-teaching">
            <h2>Teaching</h2>
            <br>
            <div>
                @<b>University of Virginia</b>
                <br>
                Computer Graphics
                <br>
                Operating Systems
                <br>
                Information Technology
                <br>
                <br>
                @<b>University of Pennsylvania</b>
                <br>
                Probability Theory
            </div>
        </div>
    </p>

</body></html>
