<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="keywords" content="Ning Yu; 于宁; Computer Vision; Visual Security; Deep Generative Modeling; University of Maryland; UMD; Max Planck Institute for Informatics; MPI-INF">
<link rel="author" href="https://ningyu1991.github.io/">

<title>Ning Yu's Homepage</title>

<style>
@media screen and (max-device-width: 480px){
  body{
    -webkit-text-size-adjust: none;
  }
}
p { font-size : 16px; }
h1 { font-size : 34px; margin : 0; padding : 0; }
h2 { font-size : 20px; margin : 0; padding : 0; }
h3 { font-size : 18px; margin : 8; padding : 0; }
body { padding : 0; font-family : Arial; font-size : 16px; background-color : #fff; }
.title { width : 700px; margin : 20px auto; }
.container { width : 1024px; margin : 20px auto; border-radius: 10px;  background-color : #fff; padding : 20px;  clear:both;}
#bio {
    padding-top : 30px;
}
#me { border : 0 solid black; margin-bottom : 50px; border-radius : 10px; }
#sidebar { margin-left : 25px; border : 0 solid black; float : right; margin-bottom : 0;}
a { text-decoration : none; }
a:hover { text-decoration : underline; }
a, a:visited { color : #0050e7; }
.publogo { width: 100 px; margin-right : 20px; float : left; border : 0;}
.publication { clear : left; padding-bottom : 0px; }
.publication p { height : 100px; padding-top : 5px;}
.publication strong a { color : #0000A0; }
.publication .links a { margin-right : 20px; }
.codelogo { margin-right : 10px; float : left; border : 0;}
.code { clear : left; padding-bottom : 10px; vertical-align :middle;} 
.code .download a { display : block; margin : 0 15px; float : left;}
.code strong a { color : #000; }
.external a { margin : 0 10px; }
.external a.first { margin : 0 10px 0 0; }
</style>

<script async="" src="./homepage_files/analytics.js"></script>
</head>

<body>
    <div class="title">
        <div id="sidebar"><img src="./homepage_files/me.jpg" vspace="50 px" width="270 px" id="me" itemprop="photo"></div>
        <div id="bio">

            <br>

            <h1>
                <span itemprop="name">Ning Yu <font size="5">于宁</font> </span>
            </h1>

            <br>

            <p style="line-height:23px;">
                Lead Research Scientist
                <br>
                Netflix <a href="https://www.eyelinestudios.com/" target="_blank" rel="nofollow">Eyeline Studios</a> and <a href="https://scanlinevfx.com/" target="_blank" rel="nofollow">ScanlineVFX</a>
                <br>
                ningyu.hust at gmail.com
                <br>
                <br>
            </p>
            <p class="external">
                <a href="https://scholar.google.com/citations?user=TaJND9YAAAAJ&hl=en" class="first" target="_blank" rel="nofollow">Google Scholar</a>|
                <a href="https://github.com/ningyu1991" target="_blank" rel="nofollow">GitHub</a>|
                <a href="https://www.linkedin.com/in/ning-yu-51b31087" target="_blank" rel="nofollow">LinkedIn</a>|
                <a href="https://twitter.com/realNingYu" target="_blank" rel="nofollow">Twitter</a>
                <br>
                <br>
                <a href="#sect-news" class="first">News</a>|
                <a href="#sect-publications">Publications</a>|
                <a href="#sect-patents">Patents</a>|
                <a href="#sect-awards">Awards</a>
                <a href="#sect-mentoring" class="first">Mentoring</a>|
                <a href="#sect-chairing" class="first">Chairing</a>|
                <a href="#sect-reviewing">Reviewing</a>|
                <a href="#sect-teaching">Teaching</a>
            </p>
        </div>
    </div>

    <!--
    <div class="container">
        <p>
            <h2>Call for interns</h2>
            <br>
            I am looking for Ph.D. research interns with a background in generative models and/or multimodal learning during <b><u>summer 2024</u></b> and beyond. Each internship targets 1+ top-tier publications, has the potential to transfer to real features in Salesforce AI-empowered products, and is followed by full-time opportunities. Please email me your C.V. The hiring is <b>rolling-based</b>.
        </p>
    </div>
    -->

    <div class="container">
        <p>
            <h2>About me</h2>
            <br>
            I am the lead research scientist at Netflix Eyeline Studios, leading efforts in visual and multimodal generative AI. My research aspirations lie in controllable and customizable recreation of the world via generative AI, based on representations of multimodal contents and interactions between users and models. <!--My research aspirations lie in computer vision and visual security, with a focused lens at the bright/dark sides of multimodal generative models and vision-language learning.-->
            <br>
            <br>
            I hold a joint Ph.D. from the University of Maryland and Max Planck Institute for Informatics where I was supervised by <a href="https://lsd.umiacs.io/" target="_blank" rel="nofollow">Larry Davis</a> and <a href="https://cispa.saarland/group/fritz/" target="_blank" rel="nofollow">Mario Fritz</a>. I previously worked with Salesforce, NVIDIA, and Adobe.
            <br>
            <br>
            My selected works include <a href="https://eyeline-research.github.io/Go-with-the-Flow/" target="_blank" rel="nofollow">Go-with-the-Flow</a>, <a href="https://canqin001.github.io/UniControl-Page/" target="_blank" rel="nofollow">UniControl</a>, <a href="https://artemisp.github.io/X-InstructBLIP-page/" target="_blank" rel="nofollow">X-InstructBLIP</a>, <a href="https://github.com/salesforce/ULIP" target="_blank" rel="nofollow">ULIP-2</a>, <a href="./projects/LayoutDETR.html" target="_blank" rel="nofollow">LayoutDETR</a>, <a href="https://canqin001.github.io/GlueGen-Page" target="_blank" rel="nofollow">GlueGen</a>, <a href="https://shugerdou.github.io/hive/" target="_blank" rel="nofollow">HIVE</a>, <a href="./projects/AttentionDualContrastGAN.html" target="_blank" rel="nofollow">Contrast and Attention GANs</a>, <a href="./projects/ScalableGANFingerprints.html" target="_blank" rel="nofollow">Scalable GAN Fingerprints</a>, <a href="./projects/ArtificialGANFingerprints.html" target="_blank" rel="nofollow">Artificial GAN Fingerprints</a>, <a href="./projects/GANFingerprints.html" target="_blank" rel="nofollow">GAN Fingerprints</a>, <a href="./projects/InclusiveGAN.html" target="_blank" rel="nofollow">Inclusive GAN</a>, and <a href="./projects/TextureMixer.html" target="_blank" rel="nofollow">Texture Mixer</a>. My research was featured in media press such as Salesforce Newsletter, AlmostHuman, AIera, JiangMen, QbitAI, innovations report, and FoodPlus.
            <br>
            <br>
            I am constantly dedicated to academic services, such as serving as (Lead) Area Chair for CVPR, ICCV, ECCV, NeurIPS, ICLR, ICML, AAAI, AISTATS, WACV, and BMVC.
            <br>
            <br>
            I am a recipient of the <a href="https://www.csaw.io/research" target="_blank" rel="nofollow">CSAW Europe Best Paper Finalist</a>, <a href="https://blog.twitch.tv/en/2021/01/07/introducing-our-2021-twitch-research-fellows/" target="_blank" rel="nofollow">Twitch (Amazon) Research Fellowship</a>, Microsoft Young Fellowship, <a href="https://www.qualcomm.com/invention/research/university-relations/innovation-fellowship/finalists" target="_blank" rel="nofollow">Qualcomm Innovation Fellowship Finalist x2</a>, and <a href="https://web.archive.org/web/20210422170343/http://spie.org/about-spie/press-room/mi15-news" target="_blank" rel="nofollow">SPIE Best Student Paper Finalist</a>.
        </p>
    </div>

    <div class="container">
        <p id="sect-news">
            <h2>News</h2>
            <br>
            [2025/02] <a href="https://eyeline-research.github.io/Go-with-the-Flow/" target="_blank" rel="nofollow">Go-with-the-Flow</a>, <a href="https://yiqunmei.net/lux-web/" target="_blank" rel="nofollow">Lux Post Facto</a>, <a href="https://three-bee.github.io/triplane_edit/" target="_blank" rel="nofollow">Triplane Edit</a>, and <a href="https://github.com/cc13qq/PFD" target="_blank" rel="nofollow">PFD</a> are accepted to CVPR 2025.
            <br>
            [2025/01] <a href="https://arxiv.org/pdf/2404.05083" target="_blank" rel="nofollow">DREAM</a> is accepted to NAACL 2025.
            <br>
            [2025/01] <a href="https://arxiv.org/pdf/2411.01212" target="_blank" rel="nofollow">InfRes</a> is accepted to ICLR 2025.
            <br>
            [2024/12] <a href="https://arxiv.org/pdf/2402.10941.pdf" target="_blank" rel="nofollow">Text2Data</a> is accepted to AAAI 2025. The <a href="https://github.com/SalesforceAIResearch/text2data" target="_blank" rel="nofollow">code</a> is released.
            <br>
            [2024/09] <a href="https://thisismingggg.github.io/" target="_blank" rel="nofollow">T2Vs Meet VLMs</a> is accepted to NeurIPS 2024. The <a href="https://github.com/nctu-eva-lab/VHD11K" target="_blank" rel="nofollow">code</a> and <a href="https://huggingface.co/datasets/denny3388/VHD11K/tree/main" target="_blank" rel="nofollow">data card</a> are released.
            <br>
            [2024/09] <a href="https://vlm-poison.github.io/" target="_blank" rel="nofollow">Shadowcast</a> is accepted to NeurIPS 2024. The <a href="https://github.com/umd-huang-lab/VLM-Poisoning" target="_blank" rel="nofollow">code</a> is released.
            <br>
            [2024/09] <a href="https://arxiv.org/pdf/2404.15157.pdf" target="_blank" rel="nofollow">FastTrack</a> is accepted to EMNLP 2024.
            <br>
            [2024/08] <a href="https://www.salesforceairesearch.com/opensource/xGen-MM/index.html" target="_blank" rel="nofollow">xGen-MM (BLIP-3)</a> is accepted to ECCV Workshop on EVAL-FoMo 2024. The <a href="https://github.com/salesforce/LAVIS/tree/xgen-mm" target="_blank" rel="nofollow">code</a>, <a href="https://huggingface.co/Salesforce/xgen-mm-phi3-mini-instruct-interleave-r-v1.5" target="_blank" rel="nofollow">model cards</a>, and <a href="https://huggingface.co/datasets/Salesforce/blip3-ocr-200m" target="_blank" rel="nofollow">data cards</a> are released.
            <br>
            [2024/08] <a href="https://arxiv.org/pdf/2408.11046" target="_blank" rel="nofollow">Inside the Black Box</a> is accepted to ECAI 2024.
            <br>
            [2024/07] <a href="https://www.eyelinestudios.com/research/diffrelight.html" target="_blank" rel="nofollow">DifFRelight</a> is accepted to SIGGRAPH Asia 2024.
            <br>
            [2024/07] <a href="https://onlinelibrary.wiley.com/doi/10.1002/advs.202403578" target="_blank" rel="nofollow">Vision-Language Models for Food Nutrient Screening</a> is accepted to Advanced Science 2024.
            <br>
            [2024/07] <a href="./projects/LayoutDETR.html" target="_blank" rel="nofollow">LayoutDETR</a> and <a href="https://artemisp.github.io/X-InstructBLIP-page/" target="_blank" rel="nofollow">X-InstructBLIP</a> are accepted to ECCV 2024.
            <br>
            [2024/05] <a href="https://arxiv.org/pdf/2403.13031.pdf" target="_blank" rel="nofollow">RigorLLM</a> and <a href="https://arxiv.org/pdf/2402.03181.pdf" target="_blank" rel="nofollow">C-RAG</a> are accepted to ICML 2024.
            <br>
            [2024/04] <a href="https://www.sciencedirect.com/science/article/abs/pii/S092422442400164X" target="_blank" rel="nofollow">LLMs in Food Science</a> is accepted to Trends in Food Science & Technology 2024.
            <br>
            [2024/03] <a href="https://arxiv.org/pdf/2210.04802.pdf" target="_blank" rel="nofollow">SimSCOOD</a> is accepted to NAACL 2024. The <a href="https://github.com/hajipour/SimSCOOD" target="_blank" rel="nofollow">code</a> is released.
            <br>
            [2024/02] <a href="https://shugerdou.github.io/hive/" target="_blank" rel="nofollow">HIVE</a> and <a href="https://github.com/salesforce/ULIP" target="_blank" rel="nofollow">ULIP-2</a> are accepted to CVPR 2024.
            <br>
            [2024/01] <a href="https://arxiv.org/pdf/2301.02650.pdf" target="_blank" rel="nofollow">Model-Agnostic Hierarchical Attention for 3D Object Detection</a> is accepted to ICRA 2024.
            <br>
            [2023/11] <a href="https://openreview.net/pdf?id=WLw1oDGR2Q" target="_blank" rel="nofollow">AnchMark</a> is accepted to NeurIPS Workshop on Regulatable ML 2023.
            <br>
            [2023/09] <a href="https://arxiv.org/pdf/2305.11147.pdf" target="_blank" rel="nofollow">UniControl</a> is accepted to NeurIPS 2023. The <a href="https://github.com/salesforce/UniControl" target="_blank" rel="nofollow">code</a> is released.
            <br>
            [2023/07] <a href="https://arxiv.org/pdf/2303.10056.pdf" target="_blank" rel="nofollow">GlueGen</a> is accepted to ICCV 2023. The <a href="https://github.com/salesforce/GlueGen" target="_blank" rel="nofollow">code</a> is released.
            <br>
            [2023/05] <a href="https://arxiv.org/pdf/2210.06998.pdf" target="_blank" rel="nofollow">DE-FAKE</a> is accepted to CCS 2023 as in the <strong style="color: red;">CSAW Europe best paper finalist</strong>. The <a href="https://github.com/zeyangsha/De-Fake" target="_blank" rel="nofollow">code</a> is released.
            <br>
            [2023/04] <a href="https://arxiv.org/pdf/2306.07758.pdf" target="_blank" rel="nofollow">Generated Graph Detection</a> is accepted to ICML 2023. The <a href="https://github.com/Yvonnemamama/GGD" target="_blank" rel="nofollow">code</a> is released.
            <br>
            [2023/04] <a href="https://arxiv.org/pdf/2304.11359.pdf" target="_blank" rel="nofollow">SPAD</a> and <a href="https://arxiv.org/pdf/2302.00491.pdf" target="_blank" rel="nofollow">Prototype LTR</a> are accepted to IJCAI 2023.
            <br>
            [2023/04] <a href="https://arxiv.org/pdf/2304.03400.pdf" target="_blank" rel="nofollow">RoSteALS</a> is accepted to CVPR Workshop on Media Forensics 2023. The <a href="https://github.com/TuBui/RoSteALS" target="_blank" rel="nofollow">code</a> is released.
            <br>
            [2023/02] <a href="https://arxiv.org/pdf/2303.16891.pdf" target="_blank" rel="nofollow">Mask-free OVIS</a> and <a href="https://arxiv.org/pdf/2201.07513.pdf" target="_blank" rel="nofollow">Cont-Steal</a> are accepted to CVPR 2023.
            <br>
            [2022/10] <a href="https://arxiv.org/pdf/2210.00957.pdf" target="_blank" rel="nofollow">UnGANable</a> is accepted to USENIX 2023. The <a href="https://github.com/zhenglisec/UnGANable" target="_blank" rel="nofollow">code</a> is released.
            <br>
            [2022/08] <a href="https://arxiv.org/pdf/2208.03382.pdf" target="_blank" rel="nofollow">FcF Image Inpainting</a> is accepted to WACV 2023. The <a href="https://github.com/SHI-Labs/FcF-Inpainting" target="_blank" rel="nofollow">code</a> is released.
            <br>
            [2022/07] I am one of the <a href="https://icml.cc/Conferences/2022/Reviewers" target="_blank" rel="nofollow">top reviewers</a> and serve as a session chair for ICML 2022.
            <br>
            [2022/07] <a href="https://arxiv.org/pdf/2207.02063.pdf" target="_blank" rel="nofollow">RepMix</a> is accepted to ECCV 2022. The <a href="https://github.com/TuBui/image_attribution" target="_blank" rel="nofollow">code</a> is released.
            <br>
            [2022/05] <a href="https://www.sciencedirect.com/science/article/pii/S0308814622012055" target="_blank" rel="nofollow">Deep Learning for Food Nutrient Estimation</a> is accepted to Food Chemistry 2022.
            <br>
            [2022/04] <a href="https://arxiv.org/pdf/2208.11180.pdf" target="_blank" rel="nofollow">Multi-Exit Privacy</a> is accepted to CCS 2022. The <a href="https://github.com/zhenglisec/Multi-Exit-Privacy" target="_blank" rel="nofollow">code</a> is released.
            <br>
            [2022/04] The code for <a href="https://github.com/DingfanChen/RelaxLoss" target="_blank" rel="nofollow">RelaxLoss</a> is released.
            <br>
            [2022/01] The code for <a href="https://github.com/ningyu1991/ScalableGANFingerprints" target="_blank" rel="nofollow">Scalable GAN Fingerprints</a> is released.
            <br>
            [2022/01] <a href="https://arxiv.org/pdf/2012.08726.pdf" target="_blank" rel="nofollow">Scalable GAN Fingerprints</a> and <a href="https://openreview.net/pdf?id=FEDfGWVZYIn" target="_blank" rel="nofollow">RelaxLoss</a> are accepted to ICLR 2022 as <strong style="color: red;">Spotlight</strong>.
            <br>
            [2021/10] <a href="https://arxiv.org/pdf/2101.11080.pdf" target="_blank" rel="nofollow">VIDNet</a> is accepted to BMVC 2021. The <a href="https://github.com/pengzhou1108/VIDNet" target="_blank" rel="nofollow">code</a> is released.
            <br>
            [2021/08] I defended my Ph.D. The dissertation about <a href="https://drum.lib.umd.edu/handle/1903/27856" target="_blank" rel="nofollow">Human-Centric Deep Generative Models</a> is online.
            <br>
            [2021/08] <a href="https://www.sciencedirect.com/science/article/abs/pii/S0308814621020008" target="_blank" rel="nofollow">Computer Vision for Food Nutrient Estimation</a> is accepted to Food Chemistry 2021.
            <br>
            [2021/07] <a href="https://arxiv.org/pdf/2007.08457.pdf" target="_blank" rel="nofollow">Artificial GAN Fingerprints</a> is accepted to ICCV 2021 as <strong style="color: red;">Oral</strong>. The <a href="https://github.com/ningyu1991/ArtificialGANFingerprints" target="_blank" rel="nofollow">code</a> is released.
            <br>
            [2021/07] <a href="https://arxiv.org/pdf/2103.16748.pdf" target="_blank" rel="nofollow">Dual Contrastive Loss and Attention for GANs</a> is accepted to ICCV 2021.
            <br>
            [2021/05] <a href="https://www.sciencedirect.com/science/article/abs/pii/S0963996921003367" target="_blank" rel="nofollow">Computer Vision for Food Nutrient Prediction</a> is accepted to Food Research International 2021.
            <br>
            [2021/04] <a href="https://arxiv.org/pdf/2105.14376.pdf" target="_blank" rel="nofollow">Re-Synthesis for Deepfake Detection</a> is accepted to IJCAI 2021. The <a href="https://github.com/SSAW14/BeyondtheSpectrum" target="_blank" rel="nofollow">code</a> is released.
            <br>
            [2021/03] <a href="https://www.sciencedirect.com/science/article/abs/pii/S0889157521000570" target="_blank" rel="nofollow">AI for Food Nutrient Estimation</a> is accepted to Journal of Food Composition and Analysis 2021.
            <br>
            [2021/02] <a href="https://arxiv.org/pdf/2011.14107.pdf" target="_blank" rel="nofollow">Hijack-GAN</a> is accepted to CVPR 2021. The <a href="https://github.com/a514514772/hijackgan" target="_blank" rel="nofollow">code</a> is released.
            <br>
            [2020/08] <a href="https://arxiv.org/pdf/2004.03706.pdf" target="_blank" rel="nofollow">Class Balanced Experts for Long-Tailed Recognition</a> is accepted to GCPR 2020. The <a href="https://github.com/ssfootball04/class-balanced-experts" target="_blank" rel="nofollow">code</a> is released.
            <br>
            [2020/06] <a href="https://arxiv.org/pdf/2004.03355.pdf" target="_blank" rel="nofollow">Inclusive GAN</a> is accepted to ECCV 2020. The <a href="https://github.com/ningyu1991/InclusiveGAN" target="_blank" rel="nofollow">code</a> is released.
            <br>
            [2020/03] <a href="https://arxiv.org/pdf/1909.03935.pdf" target="_blank" rel="nofollow">GAN-Leaks</a> is accepted to CCS 2020. The <a href="https://github.com/DingfanChen/GAN-Leaks" target="_blank" rel="nofollow">code</a> is released.
        </p>
    </div>

    <div class="container">
        <p id="sect-publications">
            <h2>Publications</h2>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_GoWithTheFlow.gif" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2501.08331" target="_blank" rel="nofollow">Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise</a>
                            </strong>
                            <br>
                            <br>
                            Ryan Burgert, Yuancheng Xu, Wenqi Xian, Oliver Pilarski, Pascal Clausen, Mingming He, Li Ma, Yitong Deng, Lingxiao Li, Mohsen Mousavi, Michael Ryoo, Paul Debevec, <b><u>Ning Yu</u></b>
                            <br>
                            <br>
                            <b><em>CVPR 2025</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2501.08331" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://eyeline-research.github.io/Go-with-the-Flow/" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/Eyeline-Research/Go-with-the-Flow" target="_blank" rel="nofollow">code</a>
                                <a href="https://www.youtube.com/watch?v=78EZ99uGXLM" target="_blank" rel="nofollow">video</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_LuxPostFacto.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://yiqunmei.net/lux-web/static/pdfs/lux_post_facto.pdf" target="_blank" rel="nofollow">Lux Post Facto: Learning Portrait Performance Relighting with Conditional Video Diffusion and a Hybrid Dataset</a>
                            </strong>
                            <br>
                            <br>
                            Yiqun Mei, Mingming He, Li Ma, Julien Philip, Wenqi Xian, David George, Xueming Yu, Gabriel Dedic, Ahmet Levent Tacsel, <b><u>Ning Yu</u></b>, Vishal Patel, Paul Debevec
                            <br>
                            <br>
                            <b><em>CVPR 2025</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://yiqunmei.net/lux-web/static/pdfs/lux_post_facto.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://yiqunmei.net/lux-web/" target="_blank" rel="nofollow">project</a>
                                <a href="https://www.youtube.com/watch?v=KSuRFUB4kOY" target="_blank" rel="nofollow">video</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_TriplaneEdit.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2404.03632.pdf" target="_blank" rel="nofollow">Reference-Based 3D-Aware Image Editing with Triplane</a>
                            </strong>
                            <br>
                            <br>
                            Bahri Batuhan Bilecen, Yigit Yalin, <b><u>Ning Yu</u></b>, Aysegul Dundar
                            <br>
                            <br>
                            <b><em>CVPR 2025</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2404.03632.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://three-bee.github.io/triplane_edit/" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/three-bee/triplane_edit" target="_blank" rel="nofollow">code</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_PFD.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2405.16226" target="_blank" rel="nofollow">Detecting Adversarial Data via Perturbation Forgery</a>
                            </strong>
                            <br>
                            <br>
                            Qian Wang, Chen Li, Yuchen Luo, Hefei Ling, Ping Li, Jiazhong Chen, Shijuan Huang, <b><u>Ning Yu</u></b>
                            <br>
                            <br>
                            <b><em>CVPR 2025</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2405.16226" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/cc13qq/PFD" target="_blank" rel="nofollow">code</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_DREAM.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2404.05083" target="_blank" rel="nofollow">DREAM: Improving Video-Text Retrieval Through Relevance-Based Augmentation Using Large Foundation Models</a>
                            </strong>
                            <br>
                            <br>
                            Yimu Wang, Shuai Yuan, Xiangru Jian, Wei Pang, Mushi Wang, <b><u>Ning Yu</u></b>
                            <br>
                            <br>
                            <b><em>NAACL 2025</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2404.05083" target="_blank" rel="nofollow">pdf</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_InfRes.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2411.01212" target="_blank" rel="nofollow">Infinite-Resolution Integral Noise Warping for Diffusion Models</a>
                            </strong>
                            <br>
                            <br>
                            Yitong Deng, Winnie Lin, Lingxiao Li, Dmitriy Smirnov, Ryan Burgert, <b><u>Ning Yu</u></b>, Vincent Dedun, Mohammad H. Taghavi
                            <br>
                            <br>
                            <b><em>ICLR 2025</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2411.01212" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://yitongdeng-projects.github.io/noise_warping_webpage/" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/yitongdeng-projects/infinite_resolution_integral_noise_warping_code" target="_blank" rel="nofollow">code</a>
                                <a href="https://www.youtube.com/watch?v=l0Xu008M3DM" target="_blank" rel="nofollow">video</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_Text2Data.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2402.10941.pdf" target="_blank" rel="nofollow">Text2Data: Low-Resource Data Generation with Textual Control</a>
                            </strong>
                            <br>
                            <br>
                            Shiyu Wang, Yihao Feng, Tian Lan, <b><u>Ning Yu</u></b>, Yu Bai, Ran Xu, Huan Wang, Caiming Xiong, Silvio Savarese
                            <br>
                            <br>
                            <b><em>AAAI 2025</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2402.10941.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/SalesforceAIResearch/text2data" target="_blank" rel="nofollow">code</a>
                                <a href="./homepage_files/poster_Text2Data.pdf" target="_blank" rel="nofollow">poster</a>
                                <a href="https://www.marktechpost.com/2025/03/09/salesforce-ai-releases-text2data-a-training-framework-for-low-resource-data-generation/" target="_blank" rel="nofollow">press</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_MIAMIM.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2408.06825" target="_blank" rel="nofollow">Membership Inference Attack Against Masked Image Modeling</a>
                            </strong>
                            <br>
                            <br>
                            Zheng Li, Xinlei He, <b><u>Ning Yu</u></b>, Yang Zhang
                            <br>
                            <br>
                            <b><em>arXiv 2024</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2408.06825" target="_blank" rel="nofollow">pdf</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_Atlas.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2408.00523" target="_blank" rel="nofollow">Jailbreaking Text-to-Image Models with LLM-Based Agents</a>
                            </strong>
                            <br>
                            <br>
                            Yingkai Dong, Zheng Li, Xiangtao Meng, <b><u>Ning Yu</u></b>, Shanqing Guo
                            <br>
                            <br>
                            <b><em>arXiv 2024</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2408.00523" target="_blank" rel="nofollow">pdf</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_JigMark.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2406.03720" target="_blank" rel="nofollow">JigMark: A Black-Box Approach for Enhancing Image Watermarks against Diffusion Model Edits</a>
                            </strong>
                            <br>
                            <br>
                            Minzhou Pan*, Yi Zeng*, <b><u>Ning Yu</u></b>, Cho-Jui Hsieh, Peter Henderson, Ruoxi Jia, Xue Lin
                            <br>
                            <br>
                            <b><em>arXiv 2024</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2406.03720" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/pmzzs/JigMark" target="_blank" rel="nofollow">code</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_VHD11K.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://thisismingggg.github.io/" target="_blank" rel="nofollow">T2Vs Meet VLMs: A Scalable Multimodal Dataset for Visual Harmfulness Recognition</a>
                            </strong>
                            <br>
                            <br>
                            Chen Yeh*, You-Ming Chang*, Wei-Chen Chiu, <b><u>Ning Yu</u></b>
                            <br>
                            <br>
                            <b><em>NeurIPS 2024</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2409.19734" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://thisismingggg.github.io/" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/nctu-eva-lab/VHD11K" target="_blank" rel="nofollow">code</a>
                                <a href="https://huggingface.co/datasets/denny3388/VHD11K/tree/main" target="_blank" rel="nofollow">data card</a>
                                <a href="./homepage_files/poster_VHD11K.pdf" target="_blank" rel="nofollow">poster</a>
                                <a href="https://recorder-v3.slideslive.com/#/share?share=92650&s=ae1ca94b-6fdf-4ea4-811f-384c8ce114c8" target="_blank" rel="nofollow">video</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_Shadowcast.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://vlm-poison.github.io/" target="_blank" rel="nofollow">Shadowcast: Stealthy Data Poisoning Attacks Against Vision-language Models</a>
                            </strong>
                            <br>
                            <br>
                            Yuancheng Xu, Jiarui Yao, Manli Shu, Yanchao Sun, Zichu Wu, <b><u>Ning Yu</u></b>, Tom Goldstein, Furong Huang
                            <br>
                            <br>
                            <b><em>NeurIPS 2024</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2402.06659.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://vlm-poison.github.io/" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/umd-huang-lab/VLM-Poisoning" target="_blank" rel="nofollow">code</a>
                                <a href="./homepage_files/poster_Shadowcast.pdf" target="_blank" rel="nofollow">poster</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_DifFRelight.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://www.eyelinestudios.com/research/diffrelight.html" target="_blank" rel="nofollow">DifFRelight: Diffusion-Based Facial Performance Relighting</a>
                            </strong>
                            <br>
                            <br>
                            Mingming He*, Pascal Clausen*, Ahmet Levent Tacsel*, Li Ma*, Oliver Pilarski*, Wenqi Xian, Laszlo Rikker, Xueming Yu, Ryan Burgert, <b><u>Ning Yu</u></b>, Paul Debevec
                            <br>
                            <br>
                            <b><em>SIGGRAPH Asia 2024</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2410.08188" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://www.eyelinestudios.com/research/diffrelight.html" target="_blank" rel="nofollow">project</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_FastTrack.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2404.15157.pdf" target="_blank" rel="nofollow">FastTrack: Fast and Accurate Fact Tracing for LLMs</a>
                            </strong>
                            <br>
                            <br>
                            Si Chen, Feiyang Kang, <b><u>Ning Yu</u></b>, Ruoxi Jia
                            <br>
                            <br>
                            <b><em>EMNLP 2024</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2404.15157.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="./homepage_files/poster_FastTrack.pdf" target="_blank" rel="nofollow">poster</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_InsideTheBlackBox.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2408.11046" target="_blank" rel="nofollow">Inside the Black Box: Detecting Data Leakage in Pre-trained Language Encoders</a>
                            </strong>
                            <br>
                            <br>
                            Yuan Xin, Zheng Li, <b><u>Ning Yu</u></b>, Dingfan Chen, Mario Fritz, Michael Backes, Yang Zhang
                            <br>
                            <br>
                            <b><em>ECAI 2024</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2408.11046" target="_blank" rel="nofollow">pdf</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_UMDFood.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://onlinelibrary.wiley.com/doi/10.1002/advs.202403578" target="_blank" rel="nofollow">Integrating Vision-Language Models for Accelerated High-Throughput Nutrition Screening</a>
                            </strong>
                            <br>
                            <br>
                            Peihua Ma*, Yixin Wu*, <b><u>Ning Yu</u></b>, Xiaoxue Jia, Yiyang He, Yang Zhang, Michael Backes, Qin Wang, Cheng-I Wei
                            <br>
                            <br>
                            <b><em>Advanced Science 2024</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://onlinelibrary.wiley.com/doi/10.1002/advs.202403578" target="_blank" rel="nofollow">pdf</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_LayoutDETR.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="./projects/LayoutDETR.html" target="_blank" rel="nofollow">LayoutDETR: Detection Transformer Is a Good Multimodal Layout Designer</a>
                            </strong>
                            <br>
                            <br>
                            <b><u>Ning Yu</u></b>, Chia-Chih Chen, Zeyuan Chen, Rui Meng, Gang Wu, Paul Josel, Juan Carlos Niebles, Caiming Xiong, Ran Xu
                            <br>
                            <br>
                            <b><em>ECCV 2024</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2212.09877.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="./projects/LayoutDETR.html" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/salesforce/LayoutDETR" target="_blank" rel="nofollow">code</a>
                                <a href="https://github.com/salesforce/BannerGen" target="_blank" rel="nofollow">BannerGen lib</a>
                                <a href="./homepage_files/poster_LayoutDETR.pdf" target="_blank" rel="nofollow">poster</a>
                                <a href="https://www.youtube.com/watch?v=0SVLiYmG-z0" target="_blank" rel="nofollow">video</a>
                                <a href="https://blog.salesforceairesearch.com/bannergen-a-library-for-multi-modality-banner-generation/" target="_blank" rel="nofollow">blog</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_XInstructBLIP.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2311.18799.pdf" target="_blank" rel="nofollow">X-InstructBLIP: A Framework for aligning X-Modal instruction-aware representations to LLMs and Emergent Cross-modal Reasoning</a>
                            </strong>
                            <br>
                            <br>
                            Artemis Panagopoulou, Le Xue*, <b><u>Ning Yu</u></b>*, Junnan Li, Dongxu Li, Shafiq Joty, Ran Xu, Silvio Savarese, Caiming Xiong, Juan Carlos Niebles
                            <br>
                            <br>
                            <b><em>ECCV 2024</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2311.18799.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://artemisp.github.io/X-InstructBLIP-page/" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/artemisp/LAVIS-XInstructBLIP/tree/main/projects/xinstructblip" target="_blank" rel="nofollow">code</a>
                                <a href="./homepage_files/poster_XInstructBLIP.pdf" target="_blank" rel="nofollow">poster</a>
                                <a href="https://mp.weixin.qq.com/s/tTfXfjQRTBN8BG7p9978GQ" target="_blank" rel="nofollow">press</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_xGen-MM.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://www.salesforceairesearch.com/opensource/xGen-MM/index.html" target="_blank" rel="nofollow">xGen-MM (BLIP-3): A Family of Open Large Multimodal Models</a>
                            </strong>
                            <br>
                            <br>
                            Le Xue*, Manli Shu*, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yutong Dai, Michael S Ryoo, Shrikant Kendre, Jieyu Zhang, Can Qin, Shu Zhang, Chia-Chih Chen, <b><u>Ning Yu</u></b>, Juntao Tan, Tulika Manoj Awalgaonkar, Shelby Heinecke, Huan Wang, Yejin Choi, Ludwig Schmidt, Zeyuan Chen, Silvio Savarese, Juan Carlos Niebles, Caiming Xiong, Ran Xu
                            <br>
                            <br>
                            <b><em>ECCV Workshop on EVAL-FoMo 2024</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2408.08872" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://www.salesforceairesearch.com/opensource/xGen-MM/index.html" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/salesforce/LAVIS/tree/xgen-mm" target="_blank" rel="nofollow">code</a>
                                <a href="https://huggingface.co/Salesforce/xgen-mm-phi3-mini-instruct-interleave-r-v1.5" target="_blank" rel="nofollow">model cards</a>
                                <a href="https://huggingface.co/datasets/Salesforce/blip3-ocr-200m" target="_blank" rel="nofollow">data cards</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_RigorLLM.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2403.13031.pdf" target="_blank" rel="nofollow">RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content</a>
                            </strong>
                            <br>
                            <br>
                            Zhuowen Yuan, Zidi Xiong, Yi Zeng, <b><u>Ning Yu</u></b>, Ruoxi Jia, Dawn Song, Bo Li
                            <br>
                            <br>
                            <b><em>ICML 2024</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2403.13031.pdf" target="_blank" rel="nofollow">pdf</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_C-RAG.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2402.03181.pdf" target="_blank" rel="nofollow">C-RAG: Certified Conformal Risks for Retrieval-Augmented Language Models</a>
                            </strong>
                            <br>
                            <br>
                            Mintong Kang, Nezihe Merve Gürel, <b><u>Ning Yu</u></b>, Dawn Song, Bo Li
                            <br>
                            <br>
                            <b><em>ICML 2024</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2402.03181.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/kangmintong/C-RAG" target="_blank" rel="nofollow">code</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_Augtriever.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2212.08841.pdf" target="_blank" rel="nofollow">AugTriever: Unsupervised Dense Retrieval and Domain Adaptation by Scalable Data Augmentation</a>
                            </strong>
                            <br>
                            <br>
                            Rui Meng, Ye Liu, Semih Yavuz, Divyansh Agarwal, Lifu Tu, <b><u>Ning Yu</u></b>, Jianguo Zhang, Meghana Bhat, Yingbo Zhou
                            <br>
                            <br>
                            <b><em>DCAI 2024</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2212.08841" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/salesforce/AugTriever" target="_blank" rel="nofollow">code</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_HIVE.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://shugerdou.github.io/hive/" target="_blank" rel="nofollow">HIVE: Harnessing Human Feedback for Instructional Visual Editing</a>
                            </strong>
                            <br>
                            <br>
                            Shu Zhang*, Xinyi Yang*, Yihao Feng*, Can Qin, Chia-Chih Chen, <b><u>Ning Yu</u></b>, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, Caiming Xiong, Ran Xu
                            <br>
                            <br>
                            <b><em>CVPR 2024</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2303.09618.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://shugerdou.github.io/hive/" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/salesforce/HIVE" target="_blank" rel="nofollow">code</a>
                                <a href="./homepage_files/poster_HIVE.pdf" target="_blank" rel="nofollow">poster</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_ULIP-2.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://github.com/salesforce/ULIP" target="_blank" rel="nofollow">ULIP-2: Towards Scalable Multimodal Pre-training For 3D Understanding</a>
                            </strong>
                            <br>
                            <br>
                            Le Xue, <b><u>Ning Yu</u></b>, Shu Zhang, Junnan Li, Roberto Martín-Martín, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, Silvio Savarese
                            <br>
                            <br>
                            <b><em>CVPR 2024</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2305.08275.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/salesforce/ULIP" target="_blank" rel="nofollow">code</a>
                                <a href="./homepage_files/poster_ULIP-2.pdf" target="_blank" rel="nofollow">poster</a>
                                <a href="https://blog.salesforceairesearch.com/ulip/" target="_blank" rel="nofollow">blog</a>
                                <a href="https://mp.weixin.qq.com/s/eckq5-0b-A34WnjYZaQhcg" target="_blank" rel="nofollow">press</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_SimSCOOD.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2210.04802.pdf" target="_blank" rel="nofollow">SimSCOOD: Systematic Analysis of Out-of-Distribution Behavior of Source Code Models</a>
                            </strong>
                            <br>
                            <br>
                            Hossein Hajipour, <b><u>Ning Yu</u></b>, Cristian-Alexandru Staicu, Mario Fritz
                            <br>
                            <br>
                            <b><em>NAACL 2024</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2210.04802.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/hajipour/SimSCOOD" target="_blank" rel="nofollow">code</a>
                                <a href="./homepage_files/poster_SimSCOOD.pdf" target="_blank" rel="nofollow">poster</a>
                                <a href="https://docs.google.com/presentation/d/1bPpQJxnHYASvBUr66t-fgdWkUmWLMLPKTUU1Jt5a6vU/edit?usp=sharing" target="_blank" rel="nofollow">slides</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_3dTransformer.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2301.02650.pdf" target="_blank" rel="nofollow">Model-Agnostic Hierarchical Attention for 3D Object Detection</a>
                            </strong>
                            <br>
                            <br>
                            Manli Shu, Le Xue, <b><u>Ning Yu</u></b>, Roberto Martín-Martín, Juan Carlos Niebles, Caiming Xiong, Ran Xu
                            <br>
                            <br>
                            <b><em>ICRA 2024</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2301.02650.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="./homepage_files/slides_3dTransformer.pptx" target="_blank" rel="nofollow">slides</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_LLMinFood.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://www.sciencedirect.com/science/article/abs/pii/S092422442400164X" target="_blank" rel="nofollow">Large Language Models in Food Science: Innovations, Applications, and Future</a>
                            </strong>
                            <br>
                            <br>
                            Peihua Ma, Shawn Tsai, Yiyang He, Xiaoxue Jia, Dongyang Zhen, <b><u>Ning Yu</u></b>, Qin Wang, Jaspreet Ahuja, Cheng-I Wei
                            <br>
                            <br>
                            <b><em>Trends in Food Science & Technology 2024</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://www.sciencedirect.com/science/article/abs/pii/S092422442400164X" target="_blank" rel="nofollow">pdf</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_MIAGM.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2310.19410.pdf" target="_blank" rel="nofollow">Generated Distributions Are All You Need for Membership Inference Attacks Against Generative Models</a>
                            </strong>
                            <br>
                            <br>
                            Minxing Zhang, <b><u>Ning Yu</u></b>, Rui Wen, Michael Backes, Yang Zhang
                            <br>
                            <br>
                            <b><em>WACV 2024</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2310.19410.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/minxingzhang/MIAGM" target="_blank" rel="nofollow">code</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_CAD.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2312.09481.pdf" target="_blank" rel="nofollow">Continual Adversarial Defense</a>
                            </strong>
                            <br>
                            <br>
                            Qian Wang, Yaoyao Liu, Hefei Ling, Yingwei Li, Qihao Liu, Ping Li, Jiazhong Chen, Alan Yuille, <b><u>Ning Yu</u></b>
                            <br>
                            <br>
                            <b><em>arXiv 2023</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2312.09481.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/cc13qq/CAD" target="_blank" rel="nofollow">code</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_AntifakePrompt.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2310.17419.pdf" target="_blank" rel="nofollow">AntifakePrompt: Prompt-Tuned Vision-Language Models are Fake Image Detectors</a>
                            </strong>
                            <br>
                            <br>
                            You-Ming Chang*, Chen Yeh*, Wei-Chen Chiu, <b><u>Ning Yu</u></b>
                            <br>
                            <br>
                            <b><em>arXiv 2023</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2310.17419.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/nctu-eva-lab/AntifakePrompt" target="_blank" rel="nofollow">code</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_PromptBackdoor.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2310.16613.pdf" target="_blank" rel="nofollow">On the Proactive Generation of Unsafe Images From Text-To-Image Models Using Benign Prompts</a>
                            </strong>
                            <br>
                            <br>
                            Yixin Wu, <b><u>Ning Yu</u></b>, Michael Backes, Yun Shen, Yang Zhang
                            <br>
                            <br>
                            <b><em>arXiv 2023</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2310.16613.pdf" target="_blank" rel="nofollow">pdf</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_UniControl.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://canqin001.github.io/UniControl-Page/" target="_blank" rel="nofollow">UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild</a>
                            </strong>
                            <br>
                            <br>
                            Can Qin, Shu Zhang, <b><u>Ning Yu</u></b>, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Carlos Niebles, Caiming Xiong, Silvio Savarese, Stefano Ermon, Yun Fu, Ran Xu
                            <br>
                            <br>
                            <b><em>NeurIPS 2023</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2305.11147.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://canqin001.github.io/UniControl-Page/" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/salesforce/UniControl" target="_blank" rel="nofollow">code</a>
                                <a href="https://huggingface.co/spaces/Robert001/UniControl-Demo" target="_blank" rel="nofollow">Huggingface space</a>
                                <a href="./homepage_files/poster_UniControl.pdf" target="_blank" rel="nofollow">poster</a>
                                <a href="https://blog.salesforceairesearch.com/unicontrol/" target="_blank" rel="nofollow">blog</a>
                                <a href="https://mp.weixin.qq.com/s/F8HXnkYhIbHZ-eSz9FsFEQ" target="_blank" rel="nofollow">press</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_AnchMark.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://openreview.net/pdf?id=WLw1oDGR2Q" target="_blank" rel="nofollow">AnchMark: Anchor-contrastive Watermarking against Generative Image Modifications</a>
                            </strong>
                            <br>
                            <br>
                            Minzhou Pan*, Yi Zeng*, Xue Lin, <b><u>Ning Yu</u></b>, Cho-Jui Hsieh, Ruoxi Jia
                            <br>
                            <br>
                            <b><em>NeurIPS Workshop on Regulatable ML 2023</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://openreview.net/pdf?id=WLw1oDGR2Q" target="_blank" rel="nofollow">pdf</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_GlueGen.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://canqin001.github.io/GlueGen-Page" target="_blank" rel="nofollow">GlueGen: Plug and Play Multi-modal Encoders for X-to-image Generation</a>
                            </strong>
                            <br>
                            <br>
                            Can Qin, <b><u>Ning Yu</u></b>, Chen Xing, Shu Zhang, Zeyuan Chen, Stefano Ermon, Yun Fu, Caiming Xiong, Ran Xu
                            <br>
                            <br>
                            <b><em>ICCV 2023</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2303.10056.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://canqin001.github.io/GlueGen-Page" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/salesforce/GlueGen" target="_blank" rel="nofollow">code</a>
                                <a href="./homepage_files/poster_GlueGen.pdf" target="_blank" rel="nofollow">poster</a>
                                <a href="https://blog.salesforceairesearch.com/gluegen/" target="_blank" rel="nofollow">blog</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_DEFAKE.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2210.06998.pdf" target="_blank" rel="nofollow">DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image Diffusion Models</a>
                            </strong>
                            <br>
                            <br>
                            Zeyang Sha, Zheng Li, <b><u>Ning Yu</u></b>, Yang Zhang
                            <br>
                            <br>
                            <b><em>CCS 2023 <strong style="color: red;">CSAW Europe best paper finalist</strong></em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2210.06998.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/zeyangsha/De-Fake" target="_blank" rel="nofollow">code</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_GGD.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2306.07758.pdf" target="_blank" rel="nofollow">Generated Graph Detection</a>
                            </strong>
                            <br>
                            <br>
                            Yihan Ma, Zhikun Zhang, <b><u>Ning Yu</u></b>, Xinlei He, Michael Backes, Yun Shen, Yang Zhang
                            <br>
                            <br>
                            <b><em>ICML 2023</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2306.07758.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/Yvonnemamama/GGD" target="_blank" rel="nofollow">code</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_SPAD.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://github.com/cc13qq/SAPD" target="_blank" rel="nofollow">Detecting Adversarial Faces Using Only Real Face Self-Perturbations</a>
                            </strong>
                            <br>
                            <br>
                            Qian Wang, Yongqin Xian, Hefei Ling, Jinyuan Zhang, Xiaorui Lin, Ping Li, Jiazhong Chen, <b><u>Ning Yu</u></b>
                            <br>
                            <br>
                            <b><em>IJCAI 2023</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2304.11359.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/cc13qq/SAPD" target="_blank" rel="nofollow">code</a>
                                <a href="./homepage_files/slides_SPAD.pptx" target="_blank" rel="nofollow">slides</a>
                                <a href="https://www.youtube.com/watch?v=BZ5WSeSBeQE" target="_blank" rel="nofollow">video</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_PrototypeClassifiers.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2302.00491.pdf" target="_blank" rel="nofollow">Learning Prototype Classifiers for Long-Tailed Recognition</a>
                            </strong>
                            <br>
                            <br>
                            Saurabh Sharma, Yongqin Xian, <b><u>Ning Yu</u></b>, Ambuj Singh
                            <br>
                            <br>
                            <b><em>IJCAI 2023</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2302.00491.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/saurabhsharma1993/prototype-classifier-ltr" target="_blank" rel="nofollow">code</a>
                                <a href="https://www.slideshare.net/SaurabhSharma917/learning-prototype-classifiers-for-longtailed-recognition" target="_blank" rel="nofollow">slides</a>
                                <a href="https://www.youtube.com/watch?v=9JB4jc1W57A" target="_blank" rel="nofollow">video</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_MaskFreeOVIS.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://vibashan.github.io/mask-free-ovis-web/" target="_blank" rel="nofollow">Mask-free OVIS: Open-Vocabulary Instance Segmentation without  Manual Mask Annotations</a>
                            </strong>
                            <br>
                            <br>
                            Vibashan VS, <b><u>Ning Yu</u></b>, Chen Xing, Can Qin, Mingfei Gao, Juan Carlos Niebles, Vishal Patel, Ran Xu
                            <br>
                            <br>
                            <b><em>CVPR 2023</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2303.16891.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://vibashan.github.io/mask-free-ovis-web/" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/Vibashan/Mask-free-OVIS" target="_blank" rel="nofollow">code</a>
                                <a href="./homepage_files/poster_MaskFreeOVIS.pdf" target="_blank" rel="nofollow">poster</a>
                                <a href="https://blog.salesforceairesearch.com/mask-free-ovis/" target="_blank" rel="nofollow">blog</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_ContSteal.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2201.07513.pdf" target="_blank" rel="nofollow">Can’t Steal? Cont-Steal! Contrastive Stealing Attacks Against Image Encoders</a>
                            </strong>
                            <br>
                            <br>
                            Zeyang Sha, Xinlei He, <b><u>Ning Yu</u></b>, Michael Backes, Yang Zhang
                            <br>
                            <br>
                            <b><em>CVPR 2023</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2201.07513.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/zeyangsha/Cont-Steal" target="_blank" rel="nofollow">code</a>
                                <a href="./homepage_files/poster_ContSteal.pdf" target="_blank" rel="nofollow">poster</a>
                                <a href="https://www.youtube.com/watch?v=RFw8X-eGprE" target="_blank" rel="nofollow">video</a>
                                <a href="./homepage_files/slides_ContSteal.pdf" target="_blank" rel="nofollow">slides</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_RoSteALS.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://github.com/TuBui/RoSteALS" target="_blank" rel="nofollow">RoSteALS: Robust Steganography using Autoencoder Latent Space</a>
                            </strong>
                            <br>
                            <br>
                            Tu Bui, Shruti Agarwal, <b><u>Ning Yu</u></b>, John Collomosse
                            <br>
                            <br>
                            <b><em>CVPR Workshop on Media Forensics 2023</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2304.03400.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/TuBui/RoSteALS" target="_blank" rel="nofollow">code</a>
                                <a href="https://huggingface.co/spaces/tubui/rosteal" target="_blank" rel="nofollow">Huggingface space</a>
                                <a href="./homepage_files/poster_RoSteALS.pdf" target="_blank" rel="nofollow">poster</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_UnGANable.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2210.00957.pdf" target="_blank" rel="nofollow">UnGANable: Defending Against GAN-based Face Manipulation</a>
                            </strong>
                            <br>
                            <br>
                            Zheng Li, <b><u>Ning Yu</u></b>, Ahmed Salem, Michael Backes, Mario Fritz, Yang Zhang
                            <br>
                            <br>
                            <b><em>USENIX 2023</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2210.00957.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/zhenglisec/UnGANable" target="_blank" rel="nofollow">code</a>
                                <a href="./homepage_files/poster_UnGANable.pdf" target="_blank" rel="nofollow">poster</a>
                                <a href="https://www.innovations-report.com/information-technology/cispa-researcher-develops-technique-to-safeguard-against-deepfakes/" target="_blank" rel="nofollow">press</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_FcFInpainting.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://praeclarumjj3.github.io/fcf-inpainting/" target="_blank" rel="nofollow">Keys to Better Image Inpainting: Structure and Texture Go Hand in Hand</a>
                            </strong>
                            <br>
                            <br>
                            Jitesh Jain*, Yuqian Zhou*, <b><u>Ning Yu</u></b>, Humphrey Shi
                            <br>
                            <br>
                            <b><em>WACV 2023</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2208.03382.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://praeclarumjj3.github.io/fcf-inpainting/" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/SHI-Labs/FcF-Inpainting" target="_blank" rel="nofollow">code</a>
                                <a href="https://colab.research.google.com/github/SHI-Labs/FcF-Inpainting/blob/main/colab/FcF_Inpainting.ipynb" target="_blank" rel="nofollow">demo</a>
                                <a href="https://huggingface.co/spaces/shi-lab/FcF-Inpainting" target="_blank" rel="nofollow">Huggingface space</a>
                                <a href="./homepage_files/poster_FcF-Inpainting.pdf" target="_blank" rel="nofollow">poster</a>
                                <a href="https://docs.google.com/presentation/d/1TPvnzGBq5G45r_-BRzLUdopjeq_KGUkQPGWDKAdaws0/edit?usp=share_link" target="_blank" rel="nofollow">slides</a>
                                <br>
                                <a href="https://helpx.adobe.com/photoshop/using/whats-new/2023-3.html" target="_blank" rel="nofollow">tech transfer: Photoshop 24.5 Remove Tool</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_ArtistsMIA.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2210.00968.pdf" target="_blank" rel="nofollow">Membership Inference Attacks Against Text-to-image Generation Models</a>
                            </strong>
                            <br>
                            <br>
                            Yixin Wu, <b><u>Ning Yu</u></b>, Zheng Li, Michael Backes, Yang Zhang
                            <br>
                            <br>
                            <b><em>arXiv 2022</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2210.00968.pdf" target="_blank" rel="nofollow">pdf</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_MultiExitPrivacy.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2208.11180.pdf" target="_blank" rel="nofollow">Auditing Membership Leakages of Multi-Exit Networks</a>
                            </strong>
                            <br>
                            <br>
                            Zheng Li, Yiyong Liu, Xinlei He, <b><u>Ning Yu</u></b>, Michael Backes, Yang Zhang
                            <br>
                            <br>
                            <b><em>CCS 2022</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2208.11180.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/zhenglisec/Multi-Exit-Privacy" target="_blank" rel="nofollow">code</a>
                                <a href="./homepage_files/slides_MultiExitPrivacy.pptx" target="_blank" rel="nofollow">slides</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_RepMix.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2207.02063.pdf" target="_blank" rel="nofollow">RepMix: Representation Mixing for Robust Attribution of Synthesized Images</a>
                            </strong>
                            <br>
                            <br>
                            Tu Bui, <b><u>Ning Yu</u></b>, John Collomosse
                            <br>
                            <br>
                            <b><em>ECCV 2022</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2207.02063.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/TuBui/image_attribution" target="_blank" rel="nofollow">code</a>
                                <a href="./homepage_files/poster_RepMix.pdf" target="_blank" rel="nofollow">poster</a>
                                <a href="https://kahlan.cvssp.org/data/Flickr25K/tubui/eccv22_repmix/5993.mp4" target="_blank" rel="nofollow">video</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_FoodNutrientPrediction2.jpg" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://www.sciencedirect.com/science/article/pii/S0308814622012055" target="_blank" rel="nofollow">Deep Learning Accurately Predicts Food Categories and Nutrients Based on Ingredient Statements</a>
                            </strong>
                            <br>
                            <br>
                            Peihua Ma, Zhikun Zhang, Ying Li,  <b><u>Ning Yu</u></b>, Jiping Sheng, Hande Küçük McGinty, Qin Wang, Jaspreet Ahuja
                            <br>
                            <br>
                            <b><em>Food Chemistry 2022</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://www.sciencedirect.com/science/article/pii/S0308814622012055" target="_blank" rel="nofollow">pdf</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_ScalableGANFingerprints.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="./projects/ScalableGANFingerprints.html" target="_blank" rel="nofollow">Responsible Disclosure of Generative Models Using Scalable Fingerprinting</a>
                            </strong>
                            <br>
                            <br>
                            <b><u>Ning Yu</u></b>*, Vladislav Skripniuk*, Dingfan Chen, Larry Davis, Mario Fritz
                            <br>
                            <br>
                            <b><em>ICLR 2022 <strong style="color: red;">Spotlight</strong></em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2012.08726.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="./projects/ScalableGANFingerprints.html" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/ningyu1991/ScalableGANFingerprints" target="_blank" rel="nofollow">code</a>
                                <a href="./homepage_files/poster_ScalableGANFingerprints.pdf" target="_blank" rel="nofollow">poster</a>
                                <a href="https://www.youtube.com/watch?v=UlpGtwEof3o" target="_blank" rel="nofollow">video</a>
                                <a href="https://mp.weixin.qq.com/s/v8hQT8VMjxe0zHybhxu2Aw" target="_blank" rel="nofollow">press</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_RelaxLoss.jpeg" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://openreview.net/pdf?id=FEDfGWVZYIn" target="_blank" rel="nofollow">RelaxLoss: Defending Membership Inference Attacks without Losing Utility</a>
                            </strong>
                            <br>
                            <br>
                            Dingfan Chen, <b><u>Ning Yu</u></b>, Mario Fritz
                            <br>
                            <br>
                            <b><em>ICLR 2022 <strong style="color: red;">Spotlight</strong></em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://openreview.net/pdf?id=FEDfGWVZYIn" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/DingfanChen/RelaxLoss" target="_blank" rel="nofollow">code</a>
                                <a href="./homepage_files/poster_RelaxLoss.pdf" target="_blank" rel="nofollow">poster</a>
                                <a href="https://www.youtube.com/watch?v=Iyu0gNC3oYE&t=5s" target="_blank" rel="nofollow">video</a>
                                <a href="https://mp.weixin.qq.com/s/Z-wAHDg7dLCy4NQMJzXXPQ" target="_blank" rel="nofollow">press</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_HumanCentricDeepGenerativeModels.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://drum.lib.umd.edu/handle/1903/27856" target="_blank" rel="nofollow">Human-Centric Deep Generative Models: The Blessing and The Curse</a>
                            </strong>
                            <br>
                            <br>
                            <b><u>Ning Yu</u></b>
                            <br>
                            <br>
                            <b><em>Ph.D. Dissertation 2021, University of Maryland and Max Planck Institute for Informatics</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://drum.lib.umd.edu/handle/1903/27856" target="_blank" rel="nofollow">pdf</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_VIDNet.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2101.11080.pdf" target="_blank" rel="nofollow">Deep Video Inpainting Detection</a>
                            </strong>
                            <br>
                            <br>
                            Peng Zhou, <b><u>Ning Yu</u></b>, Zuxuan Wu, Larry Davis, Abhinav Shrivastava, Ser-Nam Lim
                            <br>
                            <br>
                            <b><em>BMVC 2021</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2101.11080.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/pengzhou1108/VIDNet" target="_blank" rel="nofollow">code</a>
                                <a href="https://www.bmvc2021-virtualconference.com/conference/papers/paper_0398.html" target="_blank" rel="nofollow">video</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_ArtificialGANFingerprints.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="./projects/ArtificialGANFingerprints.html" target="_blank" rel="nofollow">Artificial Fingerprinting for Generative Models: Rooting Deepfake Attribution in Training Data</a>
                            </strong>
                            <br>
                            <br>
                            <b><u>Ning Yu</u></b>*, Vladislav Skripniuk*, Sahar Abdelnabi, Mario Fritz
                            <br>
                            <br>
                            <b><em>ICCV 2021 <strong style="color: red;">Oral</strong></em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2007.08457.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="./projects/ArtificialGANFingerprints.html" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/ningyu1991/ArtificialGANFingerprints" target="_blank" rel="nofollow">code</a>
                                <a href="./homepage_files/poster_ArtificialGANFingerprints.pdf" target="_blank" rel="nofollow">poster</a>
                                <a href="https://www.youtube.com/watch?v=j8bcOHhu4Lg" target="_blank" rel="nofollow">video</a>
                                <a href="https://mp.weixin.qq.com/s/PDxLWP7CYpDcD5y3aGQ7Rg" target="_blank" rel="nofollow">press</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_AttentionContrastGAN.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="./projects/AttentionDualContrastGAN.html" target="_blank" rel="nofollow">Dual Contrastive Loss and Attention for GANs</a>
                            </strong>
                            <br>
                            <br>
                            <b><u>Ning Yu</u></b>, Guilin Liu, Aysegul Dundar, Andrew Tao, Bryan Catanzaro, Larry Davis, Mario Fritz
                            <br>
                            <br>
                            <b><em>ICCV 2021</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2103.16748.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="./projects/AttentionDualContrastGAN.html" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/ningyu1991/AttentionDualContrastGAN" target="_blank" rel="nofollow">code</a>
                                <a href="./homepage_files/poster_AttentionDualContrastGAN.pdf" target="_blank" rel="nofollow">poster</a>
                                <a href="https://www.youtube.com/watch?v=hviCTQJzhd0" target="_blank" rel="nofollow">video</a>
                                <a href="https://mp.weixin.qq.com/s/vqc97WuwEMEYNuCJRQJE5A" target="_blank" rel="nofollow">press</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_ResynthesizerGANDetection.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://ssaw14.github.io/BeyondtheSpectrum/" target="_blank" rel="nofollow">Beyond the Spectrum: Detecting Deepfakes via Re-Synthesis</a>
                            </strong>
                            <br>
                            <br>
                            Yang He, <b><u>Ning Yu</u></b>, Margret Keuper, Mario Fritz
                            <br>
                            <br>
                            <b><em>IJCAI 2021</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2105.14376.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://ssaw14.github.io/BeyondtheSpectrum/" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/SSAW14/BeyondtheSpectrum" target="_blank" rel="nofollow">code</a>
                                <a href="https://www.youtube.com/watch?v=kQeREkzrrPM" target="_blank" rel="nofollow">video</a>
                                <a href="https://mp.weixin.qq.com/s/Vffg2wexqcEpJYDhXg__7g" target="_blank" rel="nofollow">press</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_HijackGAN.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://a514514772.github.io/hijackgan/" target="_blank" rel="nofollow">Hijack-GAN: Unintended-Use of Pretrained, Black-Box GANs</a>
                            </strong>
                            <br>
                            <br>
                            Hui-Po Wang, <b><u>Ning Yu</u></b>, Mario Fritz
                            <br>
                            <br>
                            <b><em>CVPR 2021</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2011.14107.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://a514514772.github.io/hijackgan/" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/a514514772/hijackgan" target="_blank" rel="nofollow">code</a>
                                <a href="https://a514514772.github.io/hijackgan/assets/poster.pdf" target="_blank" rel="nofollow">poster</a>
                                <a href="https://www.youtube.com/watch?v=M3rv4fqUoNQ" target="_blank" rel="nofollow">video</a>
                                <a href="https://mp.weixin.qq.com/s/Z7YGrGYxvV12AUt6sJUvJg" target="_blank" rel="nofollow">press</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_ChineseMarketFoodNutrientEstimation.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://www.sciencedirect.com/science/article/abs/pii/S0308814621020008" target="_blank" rel="nofollow">Application of Deep Learning for Image-based Chinese Market Food Nutrients Estimation</a>
                            </strong>
                            <br>
                            <br>
                            Peihua Ma, Chun Pong Lau, <b><u>Ning Yu</u></b>, An Li, Jiping Sheng
                            <br>
                            <br>
                            <b><em>Food Chemistry 2021</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://www.sciencedirect.com/science/article/abs/pii/S0308814621020008" target="_blank" rel="nofollow">pdf</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_ChineseFoodNutrientPrediction.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://www.sciencedirect.com/science/article/abs/pii/S0963996921003367" target="_blank" rel="nofollow">Image-based Nutrient Estimation for Chinese Dishes Using Deep Learning</a>
                            </strong>
                            <br>
                            <br>
                            Peihua Ma, Chun Pong Lau, <b><u>Ning Yu</u></b>, An Li, Ping Liu, Qin Wang, Jiping Sheng
                            <br>
                            <br>
                            <b><em>Food Research International 2021</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://www.sciencedirect.com/science/article/abs/pii/S0963996921003367" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://mp.weixin.qq.com/s/RY-PVK7ogCHbVxf5CpMStg" target="_blank" rel="nofollow">press</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_FoodNutrientPrediction.jpg" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://www.sciencedirect.com/science/article/abs/pii/S0889157521000570" target="_blank" rel="nofollow">Application of Machine Learning for Predicting Label Nutrients Using USDA Global Branded Food Products Database (BFPD)</a>
                            </strong>
                            <br>
                            <br>
                            Peihua Ma, An Li, <b><u>Ning Yu</u></b>, Ying Li, Rahul Bahadur, Qin Wang, Jaspreet Ahuja
                            <br>
                            <br>
                            <b><em>Journal of Food Composition and Analysis 2021</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://www.sciencedirect.com/science/article/abs/pii/S0889157521000570" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://mp.weixin.qq.com/s/Q_Gq1R5pRuxQQGvvrYnn_Q" target="_blank" rel="nofollow">press</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_ClassBalancedExperts.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/2004.03706.pdf" target="_blank" rel="nofollow">Long-Tailed Recognition Using Class-Balanced Experts</a>
                            </strong>
                            <br>
                            <br>
                            Saurabh Sharma, <b><u>Ning Yu</u></b>, Mario Fritz, Bernt Schiele
                            <br>
                            <br>
                            <b><em>GCPR 2020</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2004.03706.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/ssfootball04/class-balanced-experts" target="_blank" rel="nofollow">code</a>
                                <a href="https://www.youtube.com/watch?v=1rxMDoIm6oM&feature=youtu.be&t=29m58s" target="_blank" rel="nofollow">video</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_GANLeaks.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="https://arxiv.org/pdf/1909.03935.pdf" target="_blank" rel="nofollow">GAN-Leaks: A Taxonomy of Membership Inference Attacks against GANs</a>
                            </strong>
                            <br>
                            <br>
                            Dingfan Chen, <b><u>Ning Yu</u></b>, Yang Zhang, Mario Fritz
                            <br>
                            <br>
                            <b><em>CCS 2020</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/1909.03935.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="https://github.com/DingfanChen/GAN-Leaks" target="_blank" rel="nofollow">code</a>
                                <a href="./homepage_files/poster_GANLeaks.pdf" target="_blank" rel="nofollow">poster</a>
                                <a href="https://www.youtube.com/watch?v=UkOe_sGRYec" target="_blank" rel="nofollow">video (short)</a>
                                <a href="https://www.youtube.com/watch?v=APnsV8AFXZQ" target="_blank" rel="nofollow">video (full)</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_InclusiveGAN.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="./projects/InclusiveGAN.html" target="_blank" rel="nofollow">Inclusive GAN: Improving Data and Minority Coverage in Generative Models</a>
                            </strong>
                            <br>
                            <br>
                            <b><u>Ning Yu</u></b>, Ke Li, Peng Zhou, Jitendra Malik, Larry Davis, Mario Fritz
                            <br>
                            <br>
                            <b><em>ECCV 2020</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/2004.03355.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="./projects/InclusiveGAN.html" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/ningyu1991/InclusiveGAN" target="_blank" rel="nofollow">code</a>
                                <a href="https://www.youtube.com/watch?v=JbHWuLsn_zg" target="_blank" rel="nofollow">video (short)</a>
                                <a href="https://www.youtube.com/watch?v=oCb4cpsQ7do" target="_blank" rel="nofollow">video (full)</a>
                                <a href="https://mp.weixin.qq.com/s/6CCWQY8d0NoHEuMqWEp2dw" target="_blank" rel="nofollow">press</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_GANFingerprints.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="./projects/GANFingerprints.html" target="_blank" rel="nofollow">Attributing Fake Images to GANs: Learning and Analyzing GAN Fingerprints</a>
                            </strong>
                            <br>
                            <br>
                            <b><u>Ning Yu</u></b>, Larry Davis, Mario Fritz
                            <br>
                            <br>
                            <b><em>ICCV 2019</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/1811.08180.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="./projects/GANFingerprints.html" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/ningyu1991/GANFingerprints" target="_blank" rel="nofollow">code</a>
                                <a href="./homepage_files/poster_GANFingerprints.pdf" target="_blank" rel="nofollow">poster</a>
                                <a href="https://mp.weixin.qq.com/s/se1ZyR_gfzliWB5X72OZ1Q" target="_blank" rel="nofollow">press</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_TextureMixer.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="./projects/TextureMixer.html" target="_blank" rel="nofollow">Texture Mixer: A Network for Controllable Synthesis and Interpolation of Texture</a>
                            </strong>
                            <br>
                            <br>
                            <b><u>Ning Yu</u></b>, Connelly Barnes, Eli Shechtman, Sohrab Amirghodsi, Michal Lukáč
                            <br>
                            <br>
                            <b><em>CVPR 2019</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/1901.03447.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="./projects/TextureMixer.html" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/ningyu1991/TextureMixer" target="_blank" rel="nofollow">code</a>
                                <a href="./homepage_files/poster_TextureMixer.pdf" target="_blank" rel="nofollow">poster</a>
                                <a href="https://mp.weixin.qq.com/s/b_gIB_zpUCcf2a8ImKMXRA" target="_blank" rel="nofollow">press</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_DefectDetection.jpg" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="./projects/DefectDetection.html" target="_blank" rel="nofollow">Learning to Detect Multiple Photographic Defects</a>
                            </strong>
                            <br>
                            <br>
                            <b><u>Ning Yu</u></b>, Xiaohui Shen, Zhe Lin, Radomír Měch, Connelly Barnes
                            <br>
                            <br>
                            <b><em>WACV 2018</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="https://arxiv.org/pdf/1612.01635.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="./homepage_files/supp_DefectDetection.pdf" target="_blank" rel="nofollow">supp</a>
                                <a href="./projects/DefectDetection.html" target="_blank" rel="nofollow">project</a>
                                <a href="https://github.com/ningyu1991/DefectDetection" target="_blank" rel="nofollow">code</a>
                                <a href="./homepage_files/poster_DefectDetection.pdf" target="_blank" rel="nofollow">poster</a>
                                <a href="./homepage_files/slides_DefectDetection.pptx" target="_blank" rel="nofollow">slides</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_SupervoxelMRFSeg.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="./projects/SupervoxelMRFSeg.html" target="_blank" rel="nofollow">Supervoxel-Based Hierarchical Markov Random Field Framework for Multi-Atlas Segmentation</a>
                            </strong>
                            <br>
                            <br>
                            <b><u>Ning Yu</u></b>, Hongzhi Wang, Paul Yushkevich
                            <br>
                            <br>
                            <b><em>MICCAI Workshop on Patch MI 2016</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="./homepage_files/paper_SupervoxelMRFSeg.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="./projects/SupervoxelMRFSeg.html" target="_blank" rel="nofollow">project</a>
                                <a href="./homepage_files/slides_SupervoxelMRFSeg.pptx" target="_blank" rel="nofollow">slides</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_SuperpixelBreastTumorSeg.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="./projects/SuperpixelBreastTumorSeg.html" target="_blank" rel="nofollow">A Superpixel-Based Framework for Automatic Tumor Segmentation on Breast DCE-MRI</a>
                            </strong>
                            <br>
                            <br>
                            <b><u>Ning Yu</u></b>, Jia Wu, Susan Weinstein, Bilwaj Gaonkar, Brad Keller, Ahmed Ashraf, YunQing Jiang, Christos Davatzikos, Emily Conant, Despina Kontos
                            <br>
                            <br>
                            <b><em>SPIE Medical Imaging 2015 <strong style="color: red;">Oral</strong>, <strong style="color: red;">best student paper finalist</strong></em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="./homepage_files/paper_SuperpixelBreastTumorSeg.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="./projects/SuperpixelBreastTumorSeg.html" target="_blank" rel="nofollow">project</a>
                                <a href="./homepage_files/slides_SuperpixelBreastTumorSeg.pptx" target="_blank" rel="nofollow">slides</a>
                                <a href="https://web.archive.org/web/20210422170343/http://spie.org/about-spie/press-room/mi15-news" target="_blank" rel="nofollow">press</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_QuantBreastTumorReg.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="./homepage_files/paper_QuantBreastTumorReg.pdf" target="_blank" rel="nofollow">Quantification of Tumor Changes during Neoadjuvant Chemotherapy with Longitudinal Breast DCE-MRI Registration</a>
                            </strong>
                            <br>
                            <br>
                            Jia Wu, Yangming Ou, Susan Weinstein, Emily Conant, <b><u>Ning Yu</u></b>, Vahid Hoshmand, Brad Keller, Ahmed Ashraf, Mark Rosen, Angela DeMichele, Christos Davatzikos,  Despina Kontos
                            <br>
                            <br>
                            <b><em>SPIE Medical Imaging 2015</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="./homepage_files/paper_QuantBreastTumorReg.pdf" target="_blank" rel="nofollow">pdf</a>
    、                        </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_BreastTumorPattern.jpg" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="./homepage_files/paper_BreastTumorPattern.pdf" target="_blank" rel="nofollow">Tumor Heterogeneity Patterns of DCE-MRI Parametric Response Maps May Augment Early Assessment of Neoadjuvant Chemotherapy: A Pilot Study of ACRIN 6657/I-SPY 1</a>
                            </strong>
                            <br>
                            <br>
                            Jia Wu, Susan Weinstein, Andrew Oustimov, Lauren Pantalone, <b><u>Ning Yu</u></b>, Yangming Ou, Mark Rosen, Angela DeMichele, Christos Davatzikos, Despina Kontos
                            <br>
                            <br>
                            <b><em>RSNA 2015</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="./homepage_files/paper_BreastTumorPattern.pdf" target="_blank" rel="nofollow">pdf</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_InvestigationBreatTumorReg.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="./homepage_files/paper_InvestigationBreatTumorReg.pdf" target="_blank" rel="nofollow">A Feasibility Study Investigating the Use of Quantitative Measures of Spatio-Temporal Tumor Heterogeneity Derived from 4D Breast DCE-MRI Registration as a Biomarker of Response to Neoadjuvant Chemotherapy</a>
                            </strong>
                            <br>
                            <br>
                            Jia Wu, Yangming Ou, Susan Weinstein, Emily Conant, <b><u>Ning Yu</u></b>, Vahid Hoshmand, Brad Keller, Ahmed Ashraf, Mark Rosen, Angela DeMichele, Christos Davatzikos,  Despina Kontos
                            <br>
                            <br>
                            <b><em>ISMRM 2014</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="./homepage_files/paper_InvestigationBreatTumorReg.pdf" target="_blank" rel="nofollow">pdf</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
            <div class="publication">
                <table><tbody>
                    <tr>
                        <td>
                            <img src="./homepage_files/logo_FeaturePointCorrespondencesTracking.png" class="publogo" width="200 px">
                        </td>
                        <td>
                            <strong>
                                <a href="./projects/FeaturePointCorrespondencesTracking.html" target="_blank" rel="nofollow">Robust Feature Points Correspondences for Visual Object Tracking</a>
                            </strong>
                            <br>
                            <br>
                            <b><u>Ning Yu</u></b>
                            <br>
                            <br>
                            <b><em>Undergraduate Thesis 2013, Huazhong University of Science and Technology</em></b>
                            <br>
                            <br>
                            <span class="links">
                                <a href="./homepage_files/paper_FeaturePointCorrespondencesTracking.pdf" target="_blank" rel="nofollow">pdf</a>
                                <a href="./projects/FeaturePointCorrespondencesTracking.html" target="_blank" rel="nofollow">project</a>
                                <a href="./homepage_files/slides_FeaturePointCorrespondencesTracking.pptx" target="_blank" rel="nofollow">slides</a>
                            </span>
                        </td>
                    </tr>
                </tbody></table>
            </div>
            <br>
            <br>
            <br>
        </p>

    <div class="container">
        <p id="sect-patents">
            <h2>Patents</h2>
            <br>
            <div class="publication">
                <strong>
                    Systems And Methods For Controllable Data Generation From Text
                </strong>
                <br>
                Shiyu Wang, Yihao Feng, Tian Lan, <b><u>Ning Yu</u></b>, Yu Bai, Ran Xu, Huan Wang, Caiming Xiong, Silvio Savarese
                <br>
                <em>US Application No. 18,423,081</em>
            </div>
            <br>
            <div class="publication">
                <strong>
                    <a href="https://www.freepatentsonline.com/20240370718.pdf" target="_blank" rel="nofollow">Systems and Methods for Zero-shot Cross-modal Reasoning via Uni-modal Projections to Large Language Models</a>
                </strong>
                <br>
                Artemis Panagopoulou, Le Xue, <b><u>Ning Yu</u></b>, Junnan Li, Dongxu Li, Silvio Savarese, Shafiq Joty, Ran Xu, Caiming Xiong, Juan Carlos Niebles
                <br>
                <em>US Application No. 18,400,477</em>
                <br>
                <span class="links">
                    <a href="https://www.freepatentsonline.com/20240370718.pdf" target="_blank" rel="nofollow">pdf</a>
                </span>
            </div>
            <br>
            <div class="publication">
                <strong>
                    <a href="https://patentimages.storage.googleapis.com/47/ab/d1/57d94faa18c499/US20240312128A1.pdf" target="_blank" rel="nofollow">Systems and Methods for Multimodal Pretraining for Three-Dimensional Models</a>
                </strong>
                <br>
                Le Xue, <b><u>Ning Yu</u></b>, Shu Zhang, Junnan Li, Caiming Xiong, Ran Xu, Juan Carlos Niebles, Silvio Savarese
                <br>
                <em>US Application No. 18,493,035</em>
                <br>
                <span class="links">
                    <a href="https://patentimages.storage.googleapis.com/47/ab/d1/57d94faa18c499/US20240312128A1.pdf" target="_blank" rel="nofollow">pdf</a>
                </span>
            </div>
            <br>
            <div class="publication">
                <strong>
                    <a href="https://www.freepatentsonline.com/20240386623.pdf" target="_blank" rel="nofollow">Systems and Methods for Controllable Image Generation</a>
                </strong>
                <br>
                <b><u>Ning Yu</u></b>, Can Qin, Shu Zhang, Yihao Feng, Xinyi Yang, Ran Xu
                <br>
                <em>US Application No. 18,477,764</em>
                <br>
                <span class="links">
                    <a href="https://www.freepatentsonline.com/20240386623.pdf" target="_blank" rel="nofollow">pdf</a>
                </span>
            </div>
            <br>
            <div class="publication">
                <strong>
                    <a href="https://patentimages.storage.googleapis.com/71/00/56/ca4299a9c135df/US20240303882A1.pdf" target="_blank" rel="nofollow">Systems and Methods for Feedback Based Instructional Visual Editing</a>
                </strong>
                <br>
                Shu Zhang, Xinyi Yang, Yihao Feng, Chia-Chih Chen, <b><u>Ning Yu</u></b>, Ran Xu
                <br>
                <em>US Application No. 18,350,876</em>
                <br>
                <span class="links">
                    <a href="https://patentimages.storage.googleapis.com/71/00/56/ca4299a9c135df/US20240303882A1.pdf" target="_blank" rel="nofollow">pdf</a>
                </span>
            </div>
            <br>
            <div class="publication">
                <strong>
                    <a href="https://patentimages.storage.googleapis.com/28/21/ac/da385b32d1fe1b/US20240202530A1.pdf" target="_blank" rel="nofollow">Systems and Methods for Unsupervised Training in Text Retrieval Tasks</a>
                </strong>
                <br>
                Rui Meng, Yingbo Zhou, Ye Liu, Semih Yavuz, <b><u>Ning Yu</u></b>
                <br>
                <em>US Application No. 18,303,313</em>
                <br>
                <span class="links">
                    <a href="https://patentimages.storage.googleapis.com/28/21/ac/da385b32d1fe1b/US20240202530A1.pdf" target="_blank" rel="nofollow">pdf</a>
                </span>
            </div>
            <br>
            <div class="publication">
                <strong>
                    <a href="https://patentimages.storage.googleapis.com/6c/f1/ae/8bf2d748f34906/US20240185035A1.pdf" target="_blank" rel="nofollow">Systems and Methods for Text-to-Image Generation Using Language Models</a>
                </strong>
                <br>
                <b><u>Ning Yu</u></b>, Can Qin, Chen Xing, Shu Zhang, Stefano Ermon, Caiming Xiong, Ran Xu
                <br>
                <em>US Application No. 18,162,535</em>
                <br>
                <span class="links">
                    <a href="https://patentimages.storage.googleapis.com/6c/f1/ae/8bf2d748f34906/US20240185035A1.pdf" target="_blank" rel="nofollow">pdf</a>
                </span>
            </div>
            <br>
            <div class="publication">
                <strong>
                    <a href="https://www.freepatentsonline.com/20240169746.pdf" target="_blank" rel="nofollow">Systems and Methods for Attention Mechanism in Three-Dimensional Object Detection</a>
                </strong>
                <br>
                Manli Shu, Le Xue, <b><u>Ning Yu</u></b>, Roberto Martín-Martín, Juan Carlos Niebles, Caiming Xiong, Ran Xu
                <br>
                <em>US Application No. 18,161,661</em>
                <br>
                <span class="links">
                    <a href="https://www.freepatentsonline.com/20240169746.pdf" target="_blank" rel="nofollow">pdf</a>
                </span>
            </div>
            <br>
            <div class="publication">
                <strong>
                    <a href="https://patentimages.storage.googleapis.com/85/86/2e/01b03ebb7f674c/US20240104809A1.pdf" target="_blank" rel="nofollow">Systems and Methods for Multimodal Layout Designs of Digital Publications</a>
                </strong>
                <br>
                <b><u>Ning Yu</u></b>, Chia-Chih Chen, Zeyuan Chen, Caiming Xiong, Ran Xu, Juan Carlos Niebles, Rui Meng
                <br>
                <em>US Application No. 18,161,680</em>
                <br>
                <span class="links">
                    <a href="https://patentimages.storage.googleapis.com/85/86/2e/01b03ebb7f674c/US20240104809A1.pdf" target="_blank" rel="nofollow">pdf</a>
                </span>
            </div>
            <br>
            <div class="publication">
                <strong>
                    <a href="https://www.freepatentsonline.com/20240070868.pdf" target="_blank" rel="nofollow">Systems and Methods for Open-Vocabulary Instance Segmentation</a>
                </strong>
                <br>
                <b><u>Ning Yu</u></b>, Vibashan VS, Chen Xing, Juan Carlos Niebles, Ran Xu
                <br>
                <em>US Application No. 18,159,318</em>
                <br>
                <span class="links">
                    <a href="https://www.freepatentsonline.com/20240070868.pdf" target="_blank" rel="nofollow">pdf</a>
                </span>
            </div>
            <br>
            <div class="publication">
                <strong>
                    Neural Network Training Technique
                </strong>
                <br>
                Guilin Liu, <b><u>Ning Yu</u></b>, Aysegul Dundar, Andrew Tao, Bryan Catanzaro
                <br>
                <em>US Application No. 17,165,745</em>
            </div>
            <br>
            <div class="publication">
                <strong>
                    <a href="https://patentimages.storage.googleapis.com/43/18/50/b310bb01ca1ef7/US10818043.pdf" target="_blank" rel="nofollow">Texture Interpolation Using Neural Networks</a>
                </strong>
                <br>
                Connelly Barnes, Sohrab Amirghodsi, Michal Lukáč, Eli Shechtman, <b><u>Ning Yu</u></b>
                <br>
                <em>US Patent No. 10,818,043</em>
                <br>
                <span class="links">
                    <a href="https://patentimages.storage.googleapis.com/43/18/50/b310bb01ca1ef7/US10818043.pdf" target="_blank" rel="nofollow">pdf</a>
                </span>
            </div>
            <br>
            <div class="publication">
                <strong>
                    <a href="https://patentimages.storage.googleapis.com/fb/a4/32/3d3030419f6859/US10810721.pdf" target="_blank" rel="nofollow">Digital Image Defect Identification and Correction</a>
                </strong>
                <br>
                Radomír Měch, <b><u>Ning Yu</u></b>, Xiaohui Shen, Zhe Lin
                <br>
                <em>US Patent No. 10,810,721</em>
                <br>
                <span class="links">
                    <a href="https://patentimages.storage.googleapis.com/fb/a4/32/3d3030419f6859/US10810721.pdf" target="_blank" rel="nofollow">pdf</a>
                </span>
            </div>
        </div>
    </p>

    <div class="container">
        <p id="sect-awards">
            <h2>Selected Awards</h2>
            <br>
            <div>
                [2024] CSAW Europe Best Paper Finalist
                <br>
                [2022] <a href="https://icml.cc/Conferences/2022/Reviewers" target="_blank" rel="nofollow">ICML'22 Top Reviewer</a>
                <br>
                [2021] <a href="https://blog.twitch.tv/en/2021/01/07/introducing-our-2021-twitch-research-fellows/" target="_blank" rel="nofollow">Twitch (Amazon) Research Fellowship</a>
                <br>
                [2020] <a href="https://www.qualcomm.com/invention/research/university-relations/innovation-fellowship/finalists" target="_blank" rel="nofollow">Qualcomm Innovation Fellowship Finalist</a>
                <br>
                [2019] <a href="https://www.qualcomm.com/invention/research/university-relations/innovation-fellowship/finalists" target="_blank" rel="nofollow">Qualcomm Innovation Fellowship Finalist</a>
                <br>
                [2015] <a href="https://web.archive.org/web/20210422170343/http://spie.org/about-spie/press-room/mi15-news" target="_blank" rel="nofollow">SPIE Best Student Paper Finalist</a>
                <br>
                [2015] <a href="https://web.archive.org/web/20210422170343/http://spie.org/about-spie/press-room/mi15-news" target="_blank" rel="nofollow">SPIE Travel Scholarship</a>
                <br>
                [2012] Chinese National Fellowship
                <br>
                [2012] Microsoft Young Fellowship
                <br>
                [2012] Meritorious Winner of the Mathematical Contest in Modeling
                <br>
                [2011] 2nd Prize of the Chinese Mathematical Contest in Modeling
            </div>
        </div>
    </p>

    <div class="container">
        <p id="sect-mentoring">
            <h2>Mentoring</h2>
            <br>
            <div>
                <a href="https://ziqihuangg.github.io/" target="_blank" rel="nofollow">Ziqi Huang</a> @NTU
                <br>
                <a href="http://haonanqiu.com/" target="_blank" rel="nofollow">Haonan Qiu</a> @NTU
                <br>
                <a href="https://genechou.com/" target="_blank" rel="nofollow">Gene Chou</a> @Cornell
                <br>
                <a href="https://libingzeng.github.io/" target="_blank" rel="nofollow">Libing Zeng</a> @TAMU
                <br>
                <a href="https://yiqunmei.net/" target="_blank" rel="nofollow">Yiqun Mei</a> @JHU
                <br>
                <a href="https://ryanndagreat.github.io/" target="_blank" rel="nofollow">Ryan Burgert</a> @StonyBrook
                <br>
                <a href="https://mxtsecure.github.io/" target="_blank" rel="nofollow">Xiangtao Meng</a> @SDU
                <br>
                <a href="" target="_blank" rel="nofollow">Yingkai Dong</a> @SDU
                <br>
                <a href="https://yimuwangcs.github.io/" target="_blank" rel="nofollow">Yimu Wang</a> @UWaterloo
                <br>
                <a href="https://yuancheng-xu.github.io/" target="_blank" rel="nofollow">Yuancheng Xu</a> @UMD
                <br>
                <a href="https://three-bee.github.io/" target="_blank" rel="nofollow">Bahri Batuhan Bilecen</a> @Bilkent
                <br>
                <a href="https://sites.google.com/view/about-shiyuwang" target="_blank" rel="nofollow">Shiyu Wang</a> @Emory
                <br>
                <a href="https://artemisp.github.io/" target="_blank" rel="nofollow">Artemis Panagopoulou</a> @UPenn
                <br>
                <a href="" target="_blank" rel="nofollow">You-Ming Chang</a> @NCTU
                <br>
                <a href="" target="_blank" rel="nofollow">Chen Yeh</a> @NCTU
                <br>
                <a href="https://cc13qq.github.io/" target="_blank" rel="nofollow">Qian Wang</a> @HUST
                <br>
                <a href="https://minxingzhang.github.io/" target="_blank" rel="nofollow">Minxing Zhang</a> @CISPA
                <br>
                <a href="https://azshue.github.io/" target="_blank" rel="nofollow">Manli Shu</a> @UMD
                <br>
                <a href="https://vibashan.github.io/" target="_blank" rel="nofollow">Vibashan VS</a> @JHU
                <br>
                <a href="https://canqin.tech/" target="_blank" rel="nofollow">Can Qin</a> @NEU
                <br>
                <a href="https://applexy.github.io/" target="_blank" rel="nofollow">Yuan Xin</a> @CISPA
                <br>
                <a href="https://yxoh.github.io/" target="_blank" rel="nofollow">Yixin Wu</a> @CISPA
                <br>
                <a href="https://fdszy.github.io/" target="_blank" rel="nofollow">Zeyang Sha</a> @CISPA
                <br>
                <a href="https://yvonnemamama.github.io/" target="_blank" rel="nofollow">Yihan Ma</a> @CISPA
                <br>
                <a href="https://praeclarumjj3.github.io/" target="_blank" rel="nofollow">Jitesh Jain</a> @IIT
                <br>
                <a href="https://zhenglisec.github.io/" target="_blank" rel="nofollow">Zheng Li</a> @CISPA
                <br>
                <a href="https://www.linkedin.com/in/vladislav-skripniuk-8a8891143/?originalSubdomain=ru" target="_blank" rel="nofollow">Vladislav Skripniuk</a> @Audatic
                <br>
                <a href="https://a514514772.github.io/#publications" target="_blank" rel="nofollow">Hui-Po Wang</a> @CISPA
                <br>
                <a href="https://dingfanchen.github.io/homepage/" target="_blank" rel="nofollow">Dingfan Chen</a> @CISPA
                <br>
                <a href="https://dynamo.cs.ucsb.edu/people/sharma" target="_blank" rel="nofollow">Saurabh Sharma</a> @UCSB
            </div>
        </div>
    </p>

    <div class="container">
        <p id="sect-chairing">
            <h2>Chairing</h2>
            <br>
            <div>
                <table><tbody>
                    <tr>
                        <td>CVPR Lead Area Chair</td> <td>since 2025</td>
                    </tr>
                    <tr>
                        <td>CVPR Area Chair</td> <td>since 2023</td>
                    </tr>
                    <tr>
                        <td>ICCV Area Chair</td> <td>since 2023</td>
                    </tr>
                    <tr>
                        <td>ECCV Area Chair</td> <td>since 2024</td>
                    </tr>
                    <tr>
                        <td>NeurIPS Area Chair</td> <td>since 2023</td>
                    </tr>
                    <tr>
                        <td>ICLR Area Chair</td> <td>since 2024</td>
                    </tr>
                    <tr>
                        <td>ICML Area Chair</td> <td>since 2025</td>
                    </tr>
                    <tr>
                        <td>ICML Session Chair</td> <td>2022</td>
                    </tr>
                    <tr>
                        <td>AAAI Senior Program Committee</td> <td>since 2023</td>
                    </tr>
                    <tr>
                        <td>AISTATS Area Chair</td> <td>since 2023</td>
                    </tr>
                    <tr>
                        <td>BMVC Area Chair</td> <td>since 2022</td>
                    </tr>
                    <tr>
                        <td>WACV Area Chair</td> <td>since 2023</td>
                    </tr>
                </tbody></table>
            </div>
        </div>
    </p>

    <div class="container">
        <p id="sect-reviewing">
            <h2>Reviewing</h2>
            <br>
            <div>
                <table><tbody>
                    <tr>
                        <td>CVPR</td> <td>since 2020</td>
                    </tr>
                    <tr>
                        <td>ICCV</td> <td>since 2019</td>
                    </tr>
                    <tr>
                        <td>ECCV</td> <td>since 2020</td>
                    </tr>
                    <tr>
                        <td>NeurIPS</td> <td>since 2021</td>
                    </tr>
                    <tr>
                        <td>ICML</td> <td>since 2021</td>
                    </tr>
                    <tr>
                        <td>ICLR</td> <td>since 2022</td>
                    </tr>
                    <tr>
                        <td>AAAI</td> <td>since 2020</td>
                    </tr>
                    <tr>
                        <td>IJCAI</td> <td>since 2020</td>
                    </tr>
                    <tr>
                        <td>SIGGRAPH</td> <td>since 2021</td>
                    </tr>
                    <tr>
                        <td>Eurographics</td> <td>since 2020</td>
                    </tr>
                    <tr>
                        <td>ICRA</td> <td>since 2020</td>
                    </tr>
                    <tr>
                        <td>TPAMI</td> <td>since 2019</td>
                    </tr>
                    <tr>
                        <td>IJCV</td> <td>since 2021</td>
                    </tr>
                    <tr>
                        <td>TOG</td> <td>since 2020</td>
                    </tr>
                    <tr>
                        <td>PR Letters</td> <td>since 2020</td>
                    </tr>
                    <tr>
                        <td>Neurocomputing</td> <td>since 2019</td>
                    </tr>
                    <tr>
                        <td>TVCJ</td> <td>since 2018</td>
                    </tr>
                </tbody></table>
            </div>
        </div>
    </p>

    <!--
    <div class="container">
        <p id="sect-teaching">
            <h2>Teaching</h2>
            <br>
            <div>
                @<b>University of Virginia</b>
                <br>
                Computer Graphics
                <br>
                Operating Systems
                <br>
                Information Technology
                <br>
                <br>
                @<b>University of Pennsylvania</b>
                <br>
                Probability Theory
            </div>
        </div>
    </p>
    -->

</body></html>
