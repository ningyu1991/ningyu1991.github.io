<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Dual Contrastive Loss and Attention for GANs</title>

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="description" content="Generative Adversarial Networks (GANs) produce impressive results on unconditional image generation when powered with large-scale image datasets. Yet generated images are still easy to spot especially on datasets with high variance (e.g. bedroom, church). In this paper, we propose various improvements to further push the boundaries in image generation. Specifically, we propose a novel dual contrastive loss and show that, with this loss, discriminator learns more generalized and distinguishable representations to incentivize generation. In addition, we revisit attention and extensively experiment with different attention blocks in the generator. We find attention to be still an important module for successful image generation even though it was not used in the recent state-of-the-art models. Lastly, we study different attention architectures in the discriminator, and propose a reference attention mechanism. By combining the strengths of these remedies, we improve the compelling state-of-the-art Frechet Inception Distance (FID) by at least 17.5% on several benchmark datasets. We obtain even more significant improvements on compositional synthetic scenes (up to 47.5% in FID)."
>
<meta name="keywords" content="GAN; generative models; attention models; contrastive learning;">
<link rel="author" href="https://ningyu1991.github.io/">

<!-- Fonts and stuff -->
<link href="./css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="./project.css" media="screen">
<link rel="stylesheet" type="text/css" media="screen" href="./iconize.css">
<link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
<link rel="stylesheet" href="./static/css/index.css">

<style>
  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }
</style>

<script async="" src="./prettify.js"></script>
<script defer src="./static/js/fontawesome.all.min.js"></script>
<script src="./static/js/index.js"></script>

</head>

<body>
  <div id="content">
    <div id="content-inner">
      
    <div class="section head">
    <h1><a href="https://arxiv.org/pdf/2103.16748.pdf" target="_blank" rel="nofollow">Dual Contrastive Loss and Attention for GANs</a></h1>
    <h2>ICCV 2021</h2>
    <br>

    <div class="authors">
      <a href="https://ningyu1991.github.io/" target="_blank" rel="nofollow">Ning Yu</a><sup>1,2</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a href="https://liuguilin1225.github.io/" target="_blank" rel="nofollow">Guilin Liu</a><sup>3</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a href="http://www.cs.bilkent.edu.tr/~adundar/" target="_blank" rel="nofollow">Aysegul Dundar</a><sup>3,4</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a href="https://scholar.google.com/citations?user=Wel9l1wAAAAJ&hl=en" target="_blank" rel="nofollow">Andrew Tao</a><sup>3</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a href="https://ctnzr.io/" target="_blank" rel="nofollow">Bryan Catanzaro</a><sup>3</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a href="https://lsd.umiacs.io/" target="_blank" rel="nofollow">Larry Davis</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a href="https://cispa.saarland/group/fritz/" target="_blank" rel="nofollow">Mario Fritz</a><sup>5</sup>

    </div>

    <div class="affiliations">
      1. University of Maryland&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      2. Max Planck Institute for Informatics&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      3. NVIDIA&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      4. Bilkent University
      <br>
      5. CISPA Helmholtz Center for Information Security
    </div>

    <div class="column has-text-centered">
      <div class="publication-links">
        <!-- PDF Link. -->
        <span class="link-block">
          <a href="https://arxiv.org/pdf/2103.16748.pdf" target="_blank" rel="nofollow"
            class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <font size="+2"><i class="fas fa-file-pdf"></i></font>
            </span>
            <span><font size="+2">pdf</font></span>
          </a>
        </span>
        <!-- Code Link. -->
        <span class="link-block">
          <a href="https://github.com/ningyu1991/AttentionDualContrastGAN" target="_blank" rel="nofollow"
            class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <font size="+2"><i class="fab fa-github"></i></font>
            </span>
            <span><font size="+2">code</font></span>
          </a>
        </span>
      </div>
    </div>
    
</div>
      
    <center><img src="./AttentionDualContrastGAN/teaser.png" border="0" width="90%"></center>

<div class="section abstract">
    <h2>Abstract</h2>
    <br>
    Generative Adversarial Networks (GANs) produce impressive results on unconditional image generation when powered with large-scale image datasets. Yet generated images are still easy to spot especially on datasets with high variance (e.g. bedroom, church). In this paper, we propose various improvements to further push the boundaries in image generation. Specifically, we propose a novel dual contrastive loss and show that, with this loss, discriminator learns more generalized and distinguishable representations to incentivize generation. In addition, we revisit attention and extensively experiment with different attention blocks in the generator. We find attention to be still an important module for successful image generation even though it was not used in the recent state-of-the-art models. Lastly, we study different attention architectures in the discriminator, and propose a reference attention mechanism. By combining the strengths of these remedies, we improve the compelling state-of-the-art Frechet Inception Distance (FID) by at least 17.5% on several benchmark datasets. We obtain even more significant improvements on compositional synthetic scenes (up to 47.5% in FID).
</div>

<div class="section results">
  <h2>Results</h2>
  <h3>Qualitative results (see more in the paper)</h3>
  <center><img src="./AttentionDualContrastGAN/samples.png" border="0" width="90%"></center>
  <h3>Attention maps (see more in the paper)</h3>
  <center><img src="./AttentionDualContrastGAN/attention_arrow.png" border="0" width="60%"></center>
  <h3>FID results</h3>
  <center><img src="./AttentionDualContrastGAN/table_FID.png" border="0" width="60%"></center>
</div>

<div class="section video">
  <h2>Video</h2>
  <br>
  <center><iframe width="840" height="560"
    src="https://www.youtube.com/embed/hviCTQJzhd0">
    </iframe></center>
</div>

<div class="section paper">
  <h2>Materials</h2>
  <center>
    <table style="border-collapse: collapse; border: none;"><tbody>
      <tr style="border: none;">
        <td style="border: none;">
          <center>
            <a href="https://arxiv.org/pdf/2103.16748.pdf" target="_blank" rel="nofollow"><img class="layered-paper-big" style="height:175px" src="./AttentionDualContrastGAN/page1.png"></a>
            <br><br><br>
            <a href="https://arxiv.org/pdf/2103.16748.pdf" target="_blank" rel="nofollow">Paper</a>
          </center>
        </td>
        <td style="border: none;">
          <center>
            <a href="../homepage_files/poster_AttentionDualContrastGAN.pdf" target="_blank" rel="nofollow"><img style="height:175px" src="./AttentionDualContrastGAN/poster_AttentionDualContrastGAN.png"></a>
            <br><br><br>
            <a href="../homepage_files/poster_AttentionDualContrastGAN.pdf" target="_blank" rel="nofollow">Poster</a>
          </center>
        </td>
      </tr>
    </tbody></table>
</center>
</div>
        
<div class="section code">
  <h2>Code</h2>
  <center>
    <a href="https://github.com/ningyu1991/AttentionDualContrastGAN" target="_blank" rel="nofollow"><img src="./AttentionDualContrastGAN/icon_GitHub.png" width="270 px"></a>
  </center>
</div>

<div class="section press">
  <h2>Press coverage</h2>
  <center>
    <a href="https://mp.weixin.qq.com/s/vqc97WuwEMEYNuCJRQJE5A" target="_blank" rel="nofollow"><img src="./AttentionDualContrastGAN/press_AttentionDualContrastGAN.png" width="270 px"></a>
    <br>
    <a href="https://mp.weixin.qq.com/s/vqc97WuwEMEYNuCJRQJE5A" target="_blank" rel="nofollow">thejiangmen Academia News</a>
  </center>
</div>

<div class="section citation">
  <h2>Citation</h2>
  <div class="section bibtex">
    <pre>@inproceedings{yu2021dual,
  author={Yu, Ning and Liu, Guilin and Dundar, Aysegul and Tao, Andrew and Catanzaro, Bryan and Davis, Larry and Fritz, Mario},
  title={Dual Contrastive Loss and Attention for GANs},
  booktitle = {IEEE International Conference on Computer Vision (ICCV)},
  year={2021}
}</pre>
  </div>
</div>

<div class="section acknowledgement">
  <h2>Acknowledgement</h2>
  <br>
  We thank <a href="https://research.nvidia.com/person/tero-karras" target="_blank" rel="nofollow">Tero Karras</a>, <a href="https://xunhuang.me/" target="_blank" rel="nofollow">Xun Huang</a>, and <a href="https://www.homepages.ucl.ac.uk/~ucactri/" target="_blank" rel="nofollow">Tobias Ritschel</a> for constructive advice. Ning Yu was partially supported by <a href="https://blog.twitch.tv/en/2021/01/07/introducing-our-2021-twitch-research-fellows/" target="_blank" rel="nofollow">Twitch Research Fellowship</a>. This work was also partially supported by the DARPA SAIL-ON (W911NF2020009) program. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the DARPA. 
</div>

<div class="section relatedwork">
    <h2>Related Work</h2>
    <br>
    <table style="border-collapse: collapse; border: none;"><tbody>
      <tr style="border: none;">
        <td style="border: none;">
          <center><img src="./AttentionDualContrastGAN/logo_StyleGAN2.png" class="publogo" width="70 px"></center>
        </td>
        <td style="border: none;">
          <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Karras_Analyzing_and_Improving_the_Image_Quality_of_StyleGAN_CVPR_2020_paper.pdf" target="_blank" rel="nofollow">T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, T. Aila. Analyzing and improving the image quality of stylegan. CVPR 2020.</a><br>
          <b>Comment:</b> A state-of-the-art GAN baseline method that is used as our backbone.
        </td>
      </tr>
      <tr style="border: none;">
        <td style="border: none;">
          <img src="./AttentionDualContrastGAN/logo_CLIP.png" class="publogo" width="200 px">
        </td>
        <td style="border: none;">
          <a href="http://proceedings.mlr.press/v139/radford21a/radford21a.pdf" target="_blank" rel="nofollow">A. Radford, J.W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger. Learning transferable visual models from natural language supervision. ICML 2021.</a><br>
          <b>Comment:</b> The classic contrastive learning technique that is used in our loss term.
        </td>
      </tr>
      <tr style="border: none;">
        <td style="border: none;">
          <center><img src="./AttentionDualContrastGAN/logo_SAN.jpg" class="publogo" width="100 px"></center>
        </td>
        <td style="border: none;">
          <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhao_Exploring_Self-Attention_for_Image_Recognition_CVPR_2020_paper.pdf" target="_blank" rel="nofollow">H. Zhao, J. Jia, V. Koltun. Exploring self-attention for image recognition. CVPR 2020.</a><br>
          <b>Comment:</b> The state-of-the-art attention architecture that is used for our self attention and reference attention.
        </td>
      </tr>
      <tr style="border: none;">
        <td style="border: none;">
          <img src="./AttentionDualContrastGAN/logo_BigGAN.png" class="publogo" width="200 px">
        </td>
        <td style="border: none;">
          <a href="https://arxiv.org/pdf/1809.11096.pdf%20http://arxiv.org/abs/1809.11096" target="_blank" rel="nofollow">A. Brock, J. Donahue, K. Simonyan. Large scale GAN training for high fidelity natural image synthesis. ICLR 2019.</a><br>
          <b>Comment:</b> A state-of-the-art GAN baseline method that benefits from large model size and big data.
        </td>
      </tr>
      <tr style="border: none;">
        <td style="border: none;">
          <img src="./AttentionDualContrastGAN/logo_U-Net-GAN.png" class="publogo" width="200 px">
        </td>
        <td style="border: none;">
          <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Schonfeld_A_U-Net_Based_Discriminator_for_Generative_Adversarial_Networks_CVPR_2020_paper.pdf" target="_blank" rel="nofollow">E. Schonfeld, B. Schiele, A. Khoreva. A u-net based discriminator for generative adversarial networks. CVPR 2020.</a><br>
          <b>Comment:</b> A state-of-the-art GAN baseline method that is built upon BigGAN and uses U-Net to enhance its discriminator.
        </td>
      </tr>
    </tbody></table>
</div>

</body></html>
